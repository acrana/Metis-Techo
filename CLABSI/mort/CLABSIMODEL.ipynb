{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12b9881a-0f6f-479d-86b9-9d9556bede4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MIMIC IV database!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def connect_db():\n",
    "    \"\"\"Establish database connection to MIMIC IV\"\"\"\n",
    "    try:\n",
    "        engine = create_engine('postgresql://postgres:ramiel12@localhost:5432/mimiciv')\n",
    "        print(\"Successfully connected to MIMIC IV database!\")\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        print(f\"Connection failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Create connection\n",
    "engine = connect_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03adb994-dca3-4b61-8c9e-d9c858104519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created cohort with 13328 rows\n",
      "\n",
      "Cohort Summary:\n",
      "Total patients: 13328\n",
      "CLABSI cases: 114\n",
      "30-day mortality cases: 3004\n",
      "\n",
      "Detailed Statistics:\n",
      "CLABSI rate: 0.86%\n",
      "30-day mortality rate: 22.54%\n",
      "\n",
      "Line Type Distribution:\n",
      "line_type\n",
      "Multi Lumen                     7461\n",
      "PICC                            3048\n",
      "Cordis/Introducer               1169\n",
      "Dialysis                         567\n",
      "PA                               516\n",
      "Portacath                        318\n",
      "Continuous Cardiac Output PA     151\n",
      "Pre-Sep                           49\n",
      "Hickman                           39\n",
      "Triple Introducer                 10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Patient Demographics:\n",
      "Mean age: 65.4 years\n",
      "Median ICU LOS: 5.7 days\n"
     ]
    }
   ],
   "source": [
    "# Define the initial cohort query\n",
    "cohort_query = text(\"\"\"\n",
    "WITH first_icu_stays AS (\n",
    "    SELECT \n",
    "        ie.subject_id,\n",
    "        ie.hadm_id,\n",
    "        ie.stay_id,\n",
    "        ie.intime as icu_admission,\n",
    "        ie.outtime as icu_discharge,\n",
    "        EXTRACT(EPOCH FROM (ie.outtime - ie.intime))/3600 as icu_los_hours,\n",
    "        ROW_NUMBER() OVER (PARTITION BY ie.subject_id ORDER BY ie.intime) as icu_stay_number,\n",
    "        p.anchor_age + EXTRACT(EPOCH FROM adm.admittime - MAKE_TIMESTAMP(p.anchor_year, 1, 1, 0, 0, 0))/31556908.8 AS admission_age\n",
    "    FROM mimiciv_icu.icustays ie\n",
    "    INNER JOIN mimiciv_hosp.admissions adm \n",
    "        ON ie.hadm_id = adm.hadm_id\n",
    "    INNER JOIN mimiciv_hosp.patients p \n",
    "        ON ie.subject_id = p.subject_id\n",
    "    WHERE p.anchor_age + EXTRACT(EPOCH FROM adm.admittime - MAKE_TIMESTAMP(p.anchor_year, 1, 1, 0, 0, 0))/31556908.8 >= 18\n",
    "),\n",
    "first_lines AS (\n",
    "    SELECT \n",
    "        fis.*,\n",
    "        il.starttime as line_placement_time,\n",
    "        il.endtime as line_removal_time,\n",
    "        il.line_type,\n",
    "        EXTRACT(EPOCH FROM (il.endtime - il.starttime))/24/3600 as line_days,\n",
    "        ROW_NUMBER() OVER (PARTITION BY fis.stay_id ORDER BY il.starttime) as line_number\n",
    "    FROM first_icu_stays fis\n",
    "    INNER JOIN mimiciv_derived.invasive_line il \n",
    "        ON fis.stay_id = il.stay_id\n",
    "    WHERE fis.icu_stay_number = 1  -- First ICU stay only\n",
    "        AND fis.icu_los_hours >= 48  -- Survived at least 48 hours\n",
    "        AND il.line_type IN (\n",
    "            'PICC', 'Multi Lumen', 'Dialysis', 'Triple Introducer',\n",
    "            'Pre-Sep', 'Hickman', 'Portacath', 'Cordis/Introducer',\n",
    "            'Continuous Cardiac Output PA', 'PA'\n",
    "        )\n",
    "        AND EXTRACT(EPOCH FROM (il.endtime - il.starttime))/24/3600 >= 2  -- Line in place >2 days\n",
    "),\n",
    "-- Check for potential secondary BSIs\n",
    "other_infections AS (\n",
    "    SELECT DISTINCT\n",
    "        hadm_id,\n",
    "        charttime\n",
    "    FROM mimiciv_hosp.microbiologyevents\n",
    "    WHERE spec_type_desc NOT IN ('BLOOD CULTURE', '')\n",
    "        AND org_name IS NOT NULL\n",
    "),\n",
    "blood_cultures AS (\n",
    "    SELECT \n",
    "        qs.stay_id,\n",
    "        qs.subject_id,\n",
    "        qs.hadm_id,\n",
    "        me.charttime,\n",
    "        me.spec_type_desc,\n",
    "        me.org_name,\n",
    "        CASE \n",
    "            WHEN LOWER(me.org_name) SIMILAR TO '%(coagulase-negative staphylococci|staphylococcus epidermidis|staphylococcus haemolyticus|staphylococcus hominis|propionibacterium|corynebacterium|diphtheroids|bacillus species|micrococcus)%' \n",
    "                THEN 'common_commensal'\n",
    "            WHEN LOWER(me.org_name) SIMILAR TO '%(campylobacter|salmonella|shigella|listeria|vibrio|yersinia|difficile|enterohemorrhagic|enteropathogenic|blastomyces|histoplasma|coccidioides|paracoccidioides|cryptococcus|pneumocystis)%' \n",
    "                THEN 'excluded'\n",
    "            ELSE 'recognized_pathogen'\n",
    "        END as organism_type\n",
    "    FROM first_lines qs\n",
    "    INNER JOIN mimiciv_hosp.microbiologyevents me \n",
    "        ON qs.hadm_id = me.hadm_id\n",
    "    WHERE me.spec_type_desc = 'BLOOD CULTURE'\n",
    "        AND me.org_name IS NOT NULL\n",
    "        AND me.charttime > qs.line_placement_time + INTERVAL '2 days'\n",
    "        AND me.charttime <= qs.line_removal_time\n",
    "        AND qs.line_number = 1  -- Only consider first line\n",
    "        -- Exclude if other infection within ±3 days\n",
    "        AND NOT EXISTS (\n",
    "            SELECT 1 \n",
    "            FROM other_infections oi \n",
    "            WHERE oi.hadm_id = me.hadm_id \n",
    "                AND oi.charttime BETWEEN me.charttime - INTERVAL '3 days' \n",
    "                AND me.charttime + INTERVAL '3 days'\n",
    "        )\n",
    "),\n",
    "clabsi_events AS (\n",
    "    SELECT DISTINCT\n",
    "        stay_id,\n",
    "        subject_id,\n",
    "        hadm_id,\n",
    "        MIN(charttime) as infection_date\n",
    "    FROM blood_cultures bc\n",
    "    WHERE (organism_type = 'recognized_pathogen')\n",
    "       OR (organism_type = 'common_commensal' \n",
    "           AND EXISTS (\n",
    "               SELECT 1 \n",
    "               FROM blood_cultures bc2 \n",
    "               WHERE bc2.stay_id = bc.stay_id \n",
    "                   AND bc2.org_name = bc.org_name \n",
    "                   AND bc2.charttime != bc.charttime\n",
    "                   AND bc2.charttime <= bc.charttime + INTERVAL '2 days'\n",
    "           ))\n",
    "    GROUP BY stay_id, subject_id, hadm_id\n",
    "),\n",
    "mortality_outcomes AS (\n",
    "    SELECT \n",
    "        qs.stay_id,\n",
    "        qs.subject_id,\n",
    "        qs.hadm_id,\n",
    "        qs.line_placement_time,\n",
    "        CASE \n",
    "            WHEN p.dod IS NOT NULL \n",
    "                AND p.dod <= (qs.line_placement_time + INTERVAL '30 days')\n",
    "                THEN 1\n",
    "            ELSE 0\n",
    "        END as mortality_30d,\n",
    "        p.dod as death_date\n",
    "    FROM first_lines qs\n",
    "    LEFT JOIN mimiciv_hosp.patients p \n",
    "        ON qs.subject_id = p.subject_id\n",
    "    WHERE qs.line_number = 1  -- Only consider first line\n",
    ")\n",
    "SELECT \n",
    "    qs.*,\n",
    "    CASE \n",
    "        WHEN ce.stay_id IS NOT NULL THEN 1\n",
    "        ELSE 0\n",
    "    END as has_clabsi,\n",
    "    ce.infection_date,\n",
    "    mo.mortality_30d,\n",
    "    mo.death_date\n",
    "FROM first_lines qs\n",
    "LEFT JOIN clabsi_events ce \n",
    "    ON qs.stay_id = ce.stay_id\n",
    "LEFT JOIN mortality_outcomes mo \n",
    "    ON qs.stay_id = mo.stay_id\n",
    "WHERE qs.line_number = 1  -- Only include first line for each stay\n",
    "ORDER BY qs.subject_id, qs.icu_admission;\n",
    "\"\"\")\n",
    "\n",
    "# Execute query and create initial cohort\n",
    "try:\n",
    "    cohort_df = pd.read_sql(cohort_query, engine)\n",
    "    print(f\"Successfully created cohort with {len(cohort_df)} rows\")\n",
    "    print(\"\\nCohort Summary:\")\n",
    "    print(f\"Total patients: {cohort_df['subject_id'].nunique()}\")\n",
    "    print(f\"CLABSI cases: {cohort_df['has_clabsi'].sum()}\")\n",
    "    print(f\"30-day mortality cases: {cohort_df['mortality_30d'].sum()}\")\n",
    "    \n",
    "    # Additional statistics\n",
    "    print(\"\\nDetailed Statistics:\")\n",
    "    print(f\"CLABSI rate: {(cohort_df['has_clabsi'].sum() / len(cohort_df) * 100):.2f}%\")\n",
    "    print(f\"30-day mortality rate: {(cohort_df['mortality_30d'].sum() / len(cohort_df) * 100):.2f}%\")\n",
    "    print(\"\\nLine Type Distribution:\")\n",
    "    print(cohort_df['line_type'].value_counts())\n",
    "    \n",
    "    # Average age and LOS\n",
    "    print(\"\\nPatient Demographics:\")\n",
    "    print(f\"Mean age: {cohort_df['admission_age'].mean():.1f} years\")\n",
    "    print(f\"Median ICU LOS: {cohort_df['icu_los_hours'].median()/24:.1f} days\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error executing cohort query: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1808b688-cc5f-4a31-afc1-7c69c606445f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= DATA SPLIT =============\n",
      "Total rows in cohort_df: 13,328\n",
      "Training set size:       9,431\n",
      "Validation set size:     3,897\n",
      "\n",
      "Training CLABSI cases: 83 (0.88%)\n",
      "Validation CLABSI cases: 31 (0.80%)\n",
      "\n",
      "Training 30-day mortality: 2092 (22.18%)\n",
      "Validation 30-day mortality: 912 (23.40%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# NEXT CELL in your Jupyter Notebook\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Shuffle your DataFrame (optional but common)\n",
    "cohort_df = cohort_df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Create a random number column for splitting\n",
    "np.random.seed(42)  # ensures reproducible splits\n",
    "cohort_df['rand_val'] = np.random.rand(len(cohort_df))\n",
    "\n",
    "# Split into training (70%) and validation (30%) sets\n",
    "train_df = cohort_df[cohort_df['rand_val'] < 0.7].copy()\n",
    "val_df   = cohort_df[cohort_df['rand_val'] >= 0.7].copy()\n",
    "\n",
    "print(\"============= DATA SPLIT =============\")\n",
    "print(f\"Total rows in cohort_df: {len(cohort_df):,}\")\n",
    "print(f\"Training set size:       {len(train_df):,}\")\n",
    "print(f\"Validation set size:     {len(val_df):,}\")\n",
    "\n",
    "train_clabsi_count = train_df['has_clabsi'].sum()\n",
    "val_clabsi_count   = val_df['has_clabsi'].sum()\n",
    "print(f\"\\nTraining CLABSI cases: {train_clabsi_count} \"\n",
    "      f\"({train_clabsi_count/len(train_df)*100:.2f}%)\")\n",
    "print(f\"Validation CLABSI cases: {val_clabsi_count} \"\n",
    "      f\"({val_clabsi_count/len(val_df)*100:.2f}%)\")\n",
    "\n",
    "# If you'd like to also check mortality distribution:\n",
    "train_mort_count = train_df['mortality_30d'].sum()\n",
    "val_mort_count   = val_df['mortality_30d'].sum()\n",
    "print(f\"\\nTraining 30-day mortality: {train_mort_count} \"\n",
    "      f\"({train_mort_count/len(train_df)*100:.2f}%)\")\n",
    "print(f\"Validation 30-day mortality: {val_mort_count} \"\n",
    "      f\"({val_mort_count/len(val_df)*100:.2f}%)\")\n",
    "\n",
    "# Now you have train_df and val_df for further modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adaf5b37-d332-41e1-bfc2-d092c1a21119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Demographic Features Summary:\n",
      "------------------------------\n",
      "\n",
      "Training Set:\n",
      "Age (mean ± std): 65.4 ± 16.1\n",
      "\n",
      "Gender distribution:\n",
      "gender\n",
      "M    56.0\n",
      "F    44.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Ethnicity distribution:\n",
      "ethnicity\n",
      "WHITE                                        61.2\n",
      "UNKNOWN                                      15.8\n",
      "BLACK/AFRICAN AMERICAN                        6.8\n",
      "OTHER                                         3.5\n",
      "UNABLE TO OBTAIN                              1.5\n",
      "WHITE - OTHER EUROPEAN                        1.5\n",
      "HISPANIC/LATINO - PUERTO RICAN                1.2\n",
      "ASIAN                                         1.0\n",
      "ASIAN - CHINESE                               1.0\n",
      "HISPANIC/LATINO - DOMINICAN                   0.7\n",
      "HISPANIC OR LATINO                            0.7\n",
      "WHITE - RUSSIAN                               0.6\n",
      "PATIENT DECLINED TO ANSWER                    0.6\n",
      "BLACK/CAPE VERDEAN                            0.6\n",
      "BLACK/CARIBBEAN ISLAND                        0.6\n",
      "BLACK/AFRICAN                                 0.3\n",
      "PORTUGUESE                                    0.3\n",
      "ASIAN - SOUTH EAST ASIAN                      0.3\n",
      "WHITE - BRAZILIAN                             0.2\n",
      "AMERICAN INDIAN/ALASKA NATIVE                 0.2\n",
      "ASIAN - ASIAN INDIAN                          0.2\n",
      "HISPANIC/LATINO - GUATEMALAN                  0.2\n",
      "WHITE - EASTERN EUROPEAN                      0.2\n",
      "NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER     0.1\n",
      "HISPANIC/LATINO - SALVADORAN                  0.1\n",
      "MULTIPLE RACE/ETHNICITY                       0.1\n",
      "HISPANIC/LATINO - CENTRAL AMERICAN            0.1\n",
      "SOUTH AMERICAN                                0.1\n",
      "HISPANIC/LATINO - CUBAN                       0.1\n",
      "HISPANIC/LATINO - COLUMBIAN                   0.1\n",
      "HISPANIC/LATINO - HONDURAN                    0.1\n",
      "ASIAN - KOREAN                                0.1\n",
      "HISPANIC/LATINO - MEXICAN                     0.1\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Missing values:\n",
      "stay_id      0\n",
      "gender       0\n",
      "age          0\n",
      "ethnicity    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Extract stay_ids from our existing cohort splits and convert to regular Python integers\n",
    "stay_ids = tuple(int(x) for x in cohort_df['stay_id'].unique())\n",
    "\n",
    "# Demographics feature query\n",
    "demo_query = text(\"\"\"\n",
    "SELECT \n",
    "    ie.stay_id,\n",
    "    p.gender,\n",
    "    p.anchor_age + EXTRACT(EPOCH FROM adm.admittime - MAKE_TIMESTAMP(p.anchor_year, 1, 1, 0, 0, 0))/31556908.8 AS age,\n",
    "    adm.race as ethnicity\n",
    "FROM mimiciv_icu.icustays ie\n",
    "INNER JOIN mimiciv_hosp.patients p \n",
    "    ON ie.subject_id = p.subject_id\n",
    "INNER JOIN mimiciv_hosp.admissions adm \n",
    "    ON ie.hadm_id = adm.hadm_id\n",
    "WHERE ie.stay_id IN :stay_ids\n",
    "\"\"\").bindparams(stay_ids=stay_ids)\n",
    "\n",
    "# Execute query and join to cohort\n",
    "try:\n",
    "    demo_df = pd.read_sql(demo_query, engine)\n",
    "    \n",
    "    # Join to existing splits\n",
    "    train_demo = train_df.merge(demo_df, on='stay_id', how='left')\n",
    "    val_demo = val_df.merge(demo_df, on='stay_id', how='left')\n",
    "    \n",
    "    print(\"\\nDemographic Features Summary:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(\"\\nTraining Set:\")\n",
    "    print(f\"Age (mean ± std): {train_demo['age'].mean():.1f} ± {train_demo['age'].std():.1f}\")\n",
    "    print(\"\\nGender distribution:\")\n",
    "    print(train_demo['gender'].value_counts(normalize=True).multiply(100).round(1))\n",
    "    print(\"\\nEthnicity distribution:\")\n",
    "    print(train_demo['ethnicity'].value_counts(normalize=True).multiply(100).round(1))\n",
    "    \n",
    "    # Check for any missing values\n",
    "    print(\"\\nMissing values:\")\n",
    "    print(demo_df.isnull().sum())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in demographic feature extraction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db22075c-f08e-4b69-83f1-9bdf93f5527f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Laboratory Features Summary:\n",
      "------------------------------\n",
      "\n",
      "Total rows: 13328\n",
      "\n",
      "Lab Value Statistics:\n",
      "           stay_id  wbc_mean   wbc_min   wbc_max  platelet_mean  platelet_min  \\\n",
      "count     13328.00  13212.00  13212.00  13212.00       13213.00      13213.00   \n",
      "mean   34981256.06     13.67     11.83     15.61         196.15        180.21   \n",
      "std     2898093.69      9.52      8.12     11.48         106.80        104.78   \n",
      "min    30003598.00      0.10      0.10      0.10           7.33          5.00   \n",
      "25%    32428727.00      8.85      7.50      9.90         125.25        109.00   \n",
      "50%    34979983.00     12.25     10.60     13.80         178.00        162.00   \n",
      "75%    37474407.00     16.35     14.30     18.80         245.67        230.00   \n",
      "max    39999230.00    318.58    225.30    378.30        2180.50       2001.00   \n",
      "\n",
      "       platelet_max  hemoglobin_mean  hemoglobin_min  hemoglobin_max  \\\n",
      "count      13213.00         13211.00        13211.00        13211.00   \n",
      "mean         212.99            10.45            9.84           11.04   \n",
      "std          112.28             2.00            2.19            2.04   \n",
      "min            8.00             3.70            1.70            4.10   \n",
      "25%          140.00             9.00            8.20            9.60   \n",
      "50%          194.00            10.20            9.60           10.90   \n",
      "75%          263.00            11.70           11.30           12.30   \n",
      "max         2360.00            20.00           19.30           20.70   \n",
      "\n",
      "       aniongap_mean  bicarbonate_mean  creatinine_mean  chloride_mean  \\\n",
      "count       13223.00          13243.00         13246.00       13246.00   \n",
      "mean           14.53             22.30             1.60         104.65   \n",
      "std             4.04              4.45             1.63           6.62   \n",
      "min             2.00              5.07             0.10          67.00   \n",
      "25%            12.00             19.67             0.75         101.00   \n",
      "50%            14.00             22.00             1.05         105.00   \n",
      "75%            16.50             24.67             1.73         108.66   \n",
      "max            52.43             47.00            29.97         152.25   \n",
      "\n",
      "       glucose_mean  sodium_mean  potassium_mean  inr_mean   pt_mean  ptt_mean  \n",
      "count      13182.00     13244.00        13234.00  12227.00  12227.00  12175.00  \n",
      "mean         145.61       138.46            4.23      1.51     16.44     39.27  \n",
      "std           54.94         5.31            0.60      0.74      7.60     19.77  \n",
      "min           37.00       104.22            2.30      0.70      8.70     16.60  \n",
      "25%          110.21       136.00            3.80      1.17     12.90     28.00  \n",
      "50%          132.00       138.50            4.15      1.30     14.40     32.20  \n",
      "75%          165.94       141.00            4.55      1.55     16.85     41.74  \n",
      "max          872.00       176.67            9.48     19.37    144.63    150.00  \n",
      "\n",
      "Missing Values (Count and Percentage):\n",
      "wbc_mean: 116 (0.87%)\n",
      "wbc_min: 116 (0.87%)\n",
      "wbc_max: 116 (0.87%)\n",
      "platelet_mean: 115 (0.86%)\n",
      "platelet_min: 115 (0.86%)\n",
      "platelet_max: 115 (0.86%)\n",
      "hemoglobin_mean: 117 (0.88%)\n",
      "hemoglobin_min: 117 (0.88%)\n",
      "hemoglobin_max: 117 (0.88%)\n",
      "aniongap_mean: 105 (0.79%)\n",
      "bicarbonate_mean: 85 (0.64%)\n",
      "creatinine_mean: 82 (0.62%)\n",
      "chloride_mean: 82 (0.62%)\n",
      "glucose_mean: 146 (1.1%)\n",
      "sodium_mean: 84 (0.63%)\n",
      "potassium_mean: 94 (0.71%)\n",
      "inr_mean: 1101 (8.26%)\n",
      "pt_mean: 1101 (8.26%)\n",
      "ptt_mean: 1153 (8.65%)\n"
     ]
    }
   ],
   "source": [
    "# Convert stay_ids to regular Python integers\n",
    "stay_ids = tuple(int(x) for x in cohort_df['stay_id'].unique())\n",
    "\n",
    "# Lab values query for first 24 hours\n",
    "lab_query = text(\"\"\"\n",
    "WITH first_day_labs AS (\n",
    "    SELECT \n",
    "        ie.stay_id,\n",
    "        -- CBC\n",
    "        AVG(cbc.wbc) as wbc_mean,\n",
    "        MIN(cbc.wbc) as wbc_min,\n",
    "        MAX(cbc.wbc) as wbc_max,\n",
    "        AVG(cbc.platelet) as platelet_mean,\n",
    "        MIN(cbc.platelet) as platelet_min,\n",
    "        MAX(cbc.platelet) as platelet_max,\n",
    "        AVG(cbc.hemoglobin) as hemoglobin_mean,\n",
    "        MIN(cbc.hemoglobin) as hemoglobin_min,\n",
    "        MAX(cbc.hemoglobin) as hemoglobin_max,\n",
    "        -- Chemistry\n",
    "        AVG(chem.aniongap) as aniongap_mean,\n",
    "        AVG(chem.bicarbonate) as bicarbonate_mean,\n",
    "        AVG(chem.creatinine) as creatinine_mean,\n",
    "        AVG(chem.chloride) as chloride_mean,\n",
    "        AVG(chem.glucose) as glucose_mean,\n",
    "        AVG(chem.sodium) as sodium_mean,\n",
    "        AVG(chem.potassium) as potassium_mean,\n",
    "        -- Coagulation\n",
    "        AVG(coag.inr) as inr_mean,\n",
    "        AVG(coag.pt) as pt_mean,\n",
    "        AVG(coag.ptt) as ptt_mean\n",
    "    FROM mimiciv_icu.icustays ie\n",
    "    LEFT JOIN mimiciv_derived.complete_blood_count cbc\n",
    "        ON ie.subject_id = cbc.subject_id\n",
    "        AND cbc.charttime >= ie.intime\n",
    "        AND cbc.charttime <= ie.intime + INTERVAL '24 hours'\n",
    "    LEFT JOIN mimiciv_derived.chemistry chem\n",
    "        ON ie.subject_id = chem.subject_id\n",
    "        AND chem.charttime >= ie.intime\n",
    "        AND chem.charttime <= ie.intime + INTERVAL '24 hours'\n",
    "    LEFT JOIN mimiciv_derived.coagulation coag\n",
    "        ON ie.subject_id = coag.subject_id\n",
    "        AND coag.charttime >= ie.intime\n",
    "        AND coag.charttime <= ie.intime + INTERVAL '24 hours'\n",
    "    WHERE ie.stay_id IN :stay_ids\n",
    "    GROUP BY ie.stay_id\n",
    ")\n",
    "SELECT * FROM first_day_labs;\n",
    "\"\"\").bindparams(stay_ids=stay_ids)\n",
    "\n",
    "# Execute query\n",
    "try:\n",
    "    lab_df = pd.read_sql(lab_query, engine)\n",
    "    \n",
    "    print(\"\\nLaboratory Features Summary:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"\\nTotal rows: {len(lab_df)}\")\n",
    "    \n",
    "    # Basic statistics for each lab value\n",
    "    print(\"\\nLab Value Statistics:\")\n",
    "    print(lab_df.describe().round(2))\n",
    "    \n",
    "    # Missing value analysis\n",
    "    missing_counts = lab_df.isnull().sum()\n",
    "    missing_percents = (lab_df.isnull().sum() / len(lab_df) * 100).round(2)\n",
    "    print(\"\\nMissing Values (Count and Percentage):\")\n",
    "    for col, count in missing_counts.items():\n",
    "        if count > 0:\n",
    "            print(f\"{col}: {count} ({missing_percents[col]}%)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in lab feature extraction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "703b52f2-b0fe-472d-ab6b-fde3496a7c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing missing values...\n",
      "Scaling features...\n",
      "\n",
      "Processed Lab Features Summary:\n",
      "------------------------------\n",
      "\n",
      "Statistics after processing:\n",
      "        wbc_mean    wbc_min    wbc_max  platelet_mean  platelet_min  \\\n",
      "count  13328.000  13328.000  13328.000      13328.000     13328.000   \n",
      "mean       0.043      0.052      0.041          0.087         0.088   \n",
      "std        0.030      0.036      0.030          0.049         0.052   \n",
      "min        0.000      0.000      0.000          0.000         0.000   \n",
      "25%        0.028      0.033      0.026          0.054         0.053   \n",
      "50%        0.038      0.047      0.036          0.079         0.079   \n",
      "75%        0.051      0.063      0.049          0.109         0.112   \n",
      "max        1.000      1.000      1.000          1.000         1.000   \n",
      "\n",
      "       platelet_max  hemoglobin_mean  hemoglobin_min  hemoglobin_max  \\\n",
      "count     13328.000        13328.000       13328.000       13328.000   \n",
      "mean          0.087            0.414           0.463           0.418   \n",
      "std           0.048            0.122           0.124           0.123   \n",
      "min           0.000            0.000           0.000           0.000   \n",
      "25%           0.057            0.325           0.369           0.331   \n",
      "50%           0.079            0.399           0.455           0.410   \n",
      "75%           0.108            0.491           0.545           0.494   \n",
      "max           1.000            1.000           1.000           1.000   \n",
      "\n",
      "       aniongap_mean  bicarbonate_mean  creatinine_mean  chloride_mean  \\\n",
      "count      13328.000         13328.000        13328.000      13328.000   \n",
      "mean           0.248             0.411            0.050          0.442   \n",
      "std            0.080             0.106            0.054          0.077   \n",
      "min            0.000             0.000            0.000          0.000   \n",
      "25%            0.198             0.348            0.022          0.399   \n",
      "50%            0.238             0.410            0.032          0.446   \n",
      "75%            0.288             0.467            0.054          0.487   \n",
      "max            1.000             1.000            1.000          1.000   \n",
      "\n",
      "       glucose_mean  sodium_mean  potassium_mean   inr_mean    pt_mean  \\\n",
      "count     13328.000    13328.000       13328.000  13328.000  13328.000   \n",
      "mean          0.130        0.473           0.268      0.043      0.057   \n",
      "std           0.065        0.073           0.083      0.039      0.054   \n",
      "min           0.000        0.000           0.000      0.000      0.000   \n",
      "25%           0.088        0.439           0.209      0.027      0.031   \n",
      "50%           0.114        0.473           0.258      0.032      0.042   \n",
      "75%           0.154        0.508           0.314      0.046      0.060   \n",
      "max           1.000        1.000           1.000      1.000      1.000   \n",
      "\n",
      "        ptt_mean       stay_id  \n",
      "count  13328.000  1.332800e+04  \n",
      "mean       0.172  3.498126e+07  \n",
      "std        0.144  2.898094e+06  \n",
      "min        0.000  3.000360e+07  \n",
      "25%        0.088  3.242873e+07  \n",
      "50%        0.121  3.497998e+07  \n",
      "75%        0.196  3.747441e+07  \n",
      "max        1.000  3.999923e+07  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Separate lab features for processing\n",
    "lab_features = lab_df.drop('stay_id', axis=1)\n",
    "\n",
    "# Initialize KNN imputer and Min-Max scaler\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Impute missing values\n",
    "print(\"Imputing missing values...\")\n",
    "lab_features_imputed = pd.DataFrame(\n",
    "    imputer.fit_transform(lab_features),\n",
    "    columns=lab_features.columns,\n",
    "    index=lab_features.index\n",
    ")\n",
    "\n",
    "# Scale the features\n",
    "print(\"Scaling features...\")\n",
    "lab_features_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(lab_features_imputed),\n",
    "    columns=lab_features.columns,\n",
    "    index=lab_features.index\n",
    ")\n",
    "\n",
    "# Add back stay_id\n",
    "lab_features_processed = lab_features_scaled.copy()\n",
    "lab_features_processed['stay_id'] = lab_df['stay_id']\n",
    "\n",
    "print(\"\\nProcessed Lab Features Summary:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"\\nStatistics after processing:\")\n",
    "print(lab_features_processed.describe().round(3))\n",
    "\n",
    "# Store transformers for later use on validation set\n",
    "lab_transformers = {\n",
    "    'imputer': imputer,\n",
    "    'scaler': scaler\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d649a40f-f0b0-489c-817a-c8ab9aae0216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vital Signs Summary:\n",
      "------------------------------\n",
      "\n",
      "Total rows: 13328\n",
      "\n",
      "Vital Signs Statistics:\n",
      "           stay_id  heart_rate_mean  heart_rate_min  heart_rate_max  sbp_mean  \\\n",
      "count     13328.00         13301.00        13301.00        13301.00  13300.00   \n",
      "mean   34981256.06            87.36           71.88          107.34    114.95   \n",
      "std     2898093.69            16.63           16.08           21.77     15.09   \n",
      "min    30003598.00            37.58            1.00           49.00     66.08   \n",
      "25%    32428727.00            75.65           61.00           91.00    104.38   \n",
      "50%    34979983.00            85.58           70.00          105.00    112.17   \n",
      "75%    37474407.00            98.12           82.00          120.00    123.04   \n",
      "max    39999230.00           161.44          143.00          257.00    198.72   \n",
      "\n",
      "        sbp_min   sbp_max  dbp_mean   dbp_min   dbp_max  ...   mbp_max  \\\n",
      "count  13300.00  13300.00  13300.00  13300.00  13300.00  ...  13301.00   \n",
      "mean      86.62    148.42     61.27     44.50     86.82  ...    106.91   \n",
      "std       16.10     23.94     10.36     10.59     20.17  ...     29.37   \n",
      "min        4.00     81.00     28.93      1.00     39.00  ...     60.00   \n",
      "25%       77.00    132.00     54.24     38.00     73.00  ...     90.00   \n",
      "50%       86.00    145.00     60.14     44.00     84.00  ...    101.00   \n",
      "75%       95.00    161.00     67.08     50.50     97.00  ...    115.00   \n",
      "max      184.00    357.00    118.58    102.00    293.00  ...    299.00   \n",
      "\n",
      "       resp_rate_mean  resp_rate_min  resp_rate_max  temperature_mean  \\\n",
      "count        13296.00       13296.00       13296.00          12542.00   \n",
      "mean            19.93          12.57          28.75             36.90   \n",
      "std              4.14           3.97           6.74              0.65   \n",
      "min              8.70           1.00          11.00             31.80   \n",
      "25%             16.92          10.00          24.00             36.59   \n",
      "50%             19.25          12.00          28.00             36.87   \n",
      "75%             22.33          15.00          32.00             37.25   \n",
      "max             41.33          30.00          68.00             40.10   \n",
      "\n",
      "       temperature_min  temperature_max  spo2_mean  spo2_min  spo2_max  \n",
      "count         12542.00         12542.00   13300.00  13300.00  13300.00  \n",
      "mean             36.23            37.54      97.16     91.56     99.64  \n",
      "std               0.89             0.85       2.16      6.64      0.94  \n",
      "min              15.00            31.80      66.00      1.00     75.00  \n",
      "25%              36.00            37.00      95.97     90.00    100.00  \n",
      "50%              36.44            37.39      97.48     93.00    100.00  \n",
      "75%              36.72            38.00      98.78     95.00    100.00  \n",
      "max              39.80            42.30     100.00    100.00    100.00  \n",
      "\n",
      "[8 rows x 22 columns]\n",
      "\n",
      "Missing Values (Count and Percentage):\n",
      "heart_rate_mean: 27 (0.2%)\n",
      "heart_rate_min: 27 (0.2%)\n",
      "heart_rate_max: 27 (0.2%)\n",
      "sbp_mean: 28 (0.21%)\n",
      "sbp_min: 28 (0.21%)\n",
      "sbp_max: 28 (0.21%)\n",
      "dbp_mean: 28 (0.21%)\n",
      "dbp_min: 28 (0.21%)\n",
      "dbp_max: 28 (0.21%)\n",
      "mbp_mean: 27 (0.2%)\n",
      "mbp_min: 27 (0.2%)\n",
      "mbp_max: 27 (0.2%)\n",
      "resp_rate_mean: 32 (0.24%)\n",
      "resp_rate_min: 32 (0.24%)\n",
      "resp_rate_max: 32 (0.24%)\n",
      "temperature_mean: 786 (5.9%)\n",
      "temperature_min: 786 (5.9%)\n",
      "temperature_max: 786 (5.9%)\n",
      "spo2_mean: 28 (0.21%)\n",
      "spo2_min: 28 (0.21%)\n",
      "spo2_max: 28 (0.21%)\n"
     ]
    }
   ],
   "source": [
    "# Convert stay_ids to regular Python integers\n",
    "stay_ids = tuple(int(x) for x in cohort_df['stay_id'].unique())\n",
    "\n",
    "# Vital signs query for first 24 hours\n",
    "vital_query = text(\"\"\"\n",
    "WITH first_day_vitals AS (\n",
    "    SELECT \n",
    "        ie.stay_id,\n",
    "        AVG(vs.heart_rate) as heart_rate_mean,\n",
    "        MIN(vs.heart_rate) as heart_rate_min,\n",
    "        MAX(vs.heart_rate) as heart_rate_max,\n",
    "        AVG(vs.sbp) as sbp_mean,\n",
    "        MIN(vs.sbp) as sbp_min,\n",
    "        MAX(vs.sbp) as sbp_max,\n",
    "        AVG(vs.dbp) as dbp_mean,\n",
    "        MIN(vs.dbp) as dbp_min,\n",
    "        MAX(vs.dbp) as dbp_max,\n",
    "        AVG(vs.mbp) as mbp_mean,\n",
    "        MIN(vs.mbp) as mbp_min,\n",
    "        MAX(vs.mbp) as mbp_max,\n",
    "        AVG(vs.resp_rate) as resp_rate_mean,\n",
    "        MIN(vs.resp_rate) as resp_rate_min,\n",
    "        MAX(vs.resp_rate) as resp_rate_max,\n",
    "        AVG(vs.temperature) as temperature_mean,\n",
    "        MIN(vs.temperature) as temperature_min,\n",
    "        MAX(vs.temperature) as temperature_max,\n",
    "        AVG(vs.spo2) as spo2_mean,\n",
    "        MIN(vs.spo2) as spo2_min,\n",
    "        MAX(vs.spo2) as spo2_max\n",
    "    FROM mimiciv_icu.icustays ie\n",
    "    LEFT JOIN mimiciv_derived.vitalsign vs\n",
    "        ON ie.subject_id = vs.subject_id\n",
    "        AND vs.charttime >= ie.intime\n",
    "        AND vs.charttime <= ie.intime + INTERVAL '24 hours'\n",
    "    WHERE ie.stay_id IN :stay_ids\n",
    "    GROUP BY ie.stay_id\n",
    ")\n",
    "SELECT * FROM first_day_vitals;\n",
    "\"\"\").bindparams(stay_ids=stay_ids)\n",
    "\n",
    "# Execute query\n",
    "try:\n",
    "    vitals_df = pd.read_sql(vital_query, engine)\n",
    "    \n",
    "    print(\"\\nVital Signs Summary:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"\\nTotal rows: {len(vitals_df)}\")\n",
    "    \n",
    "    # Basic statistics for vitals\n",
    "    print(\"\\nVital Signs Statistics:\")\n",
    "    print(vitals_df.describe().round(2))\n",
    "    \n",
    "    # Missing value analysis\n",
    "    missing_counts = vitals_df.isnull().sum()\n",
    "    missing_percents = (vitals_df.isnull().sum() / len(vitals_df) * 100).round(2)\n",
    "    print(\"\\nMissing Values (Count and Percentage):\")\n",
    "    for col, count in missing_counts.items():\n",
    "        if count > 0:\n",
    "            print(f\"{col}: {count} ({missing_percents[col]}%)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in vital signs extraction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65e7933c-461e-4ba4-80d4-cfe20a515ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing missing values...\n",
      "Scaling features...\n",
      "\n",
      "Processed Vital Signs Summary:\n",
      "------------------------------\n",
      "\n",
      "Statistics after processing:\n",
      "       heart_rate_mean  heart_rate_min  heart_rate_max   sbp_mean    sbp_min  \\\n",
      "count        13328.000       13328.000       13328.000  13328.000  13328.000   \n",
      "mean             0.402           0.499           0.280      0.368      0.459   \n",
      "std              0.134           0.113           0.105      0.114      0.089   \n",
      "min              0.000           0.000           0.000      0.000      0.000   \n",
      "25%              0.307           0.423           0.202      0.289      0.406   \n",
      "50%              0.388           0.486           0.269      0.348      0.456   \n",
      "75%              0.489           0.570           0.341      0.429      0.506   \n",
      "max              1.000           1.000           1.000      1.000      1.000   \n",
      "\n",
      "         sbp_max   dbp_mean    dbp_min    dbp_max   mbp_mean  ...  \\\n",
      "count  13328.000  13328.000  13328.000  13328.000  13328.000  ...   \n",
      "mean       0.244      0.361      0.431      0.188      0.320  ...   \n",
      "std        0.087      0.115      0.105      0.079      0.110  ...   \n",
      "min        0.000      0.000      0.000      0.000      0.000  ...   \n",
      "25%        0.185      0.283      0.366      0.134      0.245  ...   \n",
      "50%        0.232      0.348      0.426      0.177      0.303  ...   \n",
      "75%        0.290      0.425      0.485      0.228      0.379  ...   \n",
      "max        1.000      1.000      1.000      1.000      1.000  ...   \n",
      "\n",
      "       resp_rate_mean  resp_rate_min  resp_rate_max  temperature_mean  \\\n",
      "count       13328.000      13328.000      13328.000         13328.000   \n",
      "mean            0.344          0.399          0.311             0.614   \n",
      "std             0.127          0.137          0.118             0.076   \n",
      "min             0.000          0.000          0.000             0.000   \n",
      "25%             0.252          0.310          0.228             0.577   \n",
      "50%             0.324          0.379          0.298             0.610   \n",
      "75%             0.417          0.483          0.368             0.653   \n",
      "max             1.000          1.000          1.000             1.000   \n",
      "\n",
      "       temperature_min  temperature_max  spo2_mean   spo2_min   spo2_max  \\\n",
      "count        13328.000        13328.000  13328.000  13328.000  13328.000   \n",
      "mean             0.856            0.546      0.917      0.915      0.986   \n",
      "std              0.035            0.079      0.063      0.067      0.038   \n",
      "min              0.000            0.000      0.000      0.000      0.000   \n",
      "25%              0.844            0.495      0.881      0.899      1.000   \n",
      "50%              0.865            0.532      0.926      0.929      1.000   \n",
      "75%              0.874            0.585      0.964      0.949      1.000   \n",
      "max              1.000            1.000      1.000      1.000      1.000   \n",
      "\n",
      "            stay_id  \n",
      "count  1.332800e+04  \n",
      "mean   3.498126e+07  \n",
      "std    2.898094e+06  \n",
      "min    3.000360e+07  \n",
      "25%    3.242873e+07  \n",
      "50%    3.497998e+07  \n",
      "75%    3.747441e+07  \n",
      "max    3.999923e+07  \n",
      "\n",
      "[8 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Separate vital features for processing\n",
    "vital_features = vitals_df.drop('stay_id', axis=1)\n",
    "\n",
    "# Initialize KNN imputer and Min-Max scaler\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Impute missing values\n",
    "print(\"Imputing missing values...\")\n",
    "vital_features_imputed = pd.DataFrame(\n",
    "    imputer.fit_transform(vital_features),\n",
    "    columns=vital_features.columns,\n",
    "    index=vital_features.index\n",
    ")\n",
    "\n",
    "# Scale the features\n",
    "print(\"Scaling features...\")\n",
    "vital_features_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(vital_features_imputed),\n",
    "    columns=vital_features.columns,\n",
    "    index=vital_features.index\n",
    ")\n",
    "\n",
    "# Add back stay_id\n",
    "vital_features_processed = vital_features_scaled.copy()\n",
    "vital_features_processed['stay_id'] = vitals_df['stay_id']\n",
    "\n",
    "print(\"\\nProcessed Vital Signs Summary:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"\\nStatistics after processing:\")\n",
    "print(vital_features_processed.describe().round(3))\n",
    "\n",
    "# Store transformers for later use on validation set\n",
    "vital_transformers = {\n",
    "    'imputer': imputer,\n",
    "    'scaler': scaler\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0621b41-5499-4464-8331-fc9741557584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clinical Scores Summary:\n",
      "------------------------------\n",
      "\n",
      "Total rows: 13328\n",
      "\n",
      "Scores Statistics:\n",
      "           stay_id  sofa_score  apsiii_score  sapsii_score\n",
      "count     13328.00    13328.00      13328.00      13328.00\n",
      "mean   34981256.06        6.56         52.85         42.03\n",
      "std     2898093.69        3.84         22.89         14.63\n",
      "min    30003598.00        0.00          3.00          0.00\n",
      "25%    32428727.00        4.00         36.00         32.00\n",
      "50%    34979983.00        6.00         49.00         40.00\n",
      "75%    37474407.00        9.00         66.00         51.00\n",
      "max    39999230.00       23.00        184.00        115.00\n",
      "\n",
      "Missing Values (Count and Percentage):\n"
     ]
    }
   ],
   "source": [
    "# Convert stay_ids to regular Python integers\n",
    "stay_ids = tuple(int(x) for x in cohort_df['stay_id'].unique())\n",
    "\n",
    "# Clinical scores query with the correct sofa table\n",
    "scores_query = text(\"\"\"\n",
    "SELECT \n",
    "    ie.stay_id,\n",
    "    s.sofa AS sofa_score,\n",
    "    a.apsiii AS apsiii_score,\n",
    "    sa.sapsii AS sapsii_score\n",
    "FROM mimiciv_icu.icustays ie\n",
    "LEFT JOIN mimiciv_derived.first_day_sofa s\n",
    "    ON ie.stay_id = s.stay_id\n",
    "LEFT JOIN mimiciv_derived.apsiii a\n",
    "    ON ie.stay_id = a.stay_id\n",
    "LEFT JOIN mimiciv_derived.sapsii sa\n",
    "    ON ie.stay_id = sa.stay_id\n",
    "WHERE ie.stay_id IN :stay_ids;\n",
    "\"\"\").bindparams(stay_ids=stay_ids)\n",
    "\n",
    "# Execute query\n",
    "try:\n",
    "    scores_df = pd.read_sql(scores_query, engine)\n",
    "    \n",
    "    print(\"\\nClinical Scores Summary:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"\\nTotal rows: {len(scores_df)}\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\nScores Statistics:\")\n",
    "    print(scores_df.describe().round(2))\n",
    "    \n",
    "    # Missing value analysis\n",
    "    missing_counts = scores_df.isnull().sum()\n",
    "    missing_percents = (scores_df.isnull().sum() / len(scores_df) * 100).round(2)\n",
    "    print(\"\\nMissing Values (Count and Percentage):\")\n",
    "    for col, count in missing_counts.items():\n",
    "        if count > 0:\n",
    "            print(f\"{col}: {count} ({missing_percents[col]}%)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in scores extraction: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddb29803-49a3-412d-aee6-4aa7e9e0aa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling features...\n",
      "\n",
      "Processed Clinical Scores Summary:\n",
      "------------------------------\n",
      "\n",
      "Statistics after processing:\n",
      "       sofa_score  apsiii_score  sapsii_score       stay_id\n",
      "count   13328.000     13328.000     13328.000  1.332800e+04\n",
      "mean        0.285         0.275         0.365  3.498126e+07\n",
      "std         0.167         0.126         0.127  2.898094e+06\n",
      "min         0.000         0.000         0.000  3.000360e+07\n",
      "25%         0.174         0.182         0.278  3.242873e+07\n",
      "50%         0.261         0.254         0.348  3.497998e+07\n",
      "75%         0.391         0.348         0.443  3.747441e+07\n",
      "max         1.000         1.000         1.000  3.999923e+07\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Separate score features for processing\n",
    "score_features = scores_df.drop('stay_id', axis=1)\n",
    "\n",
    "# Initialize Min-Max scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale the features\n",
    "print(\"Scaling features...\")\n",
    "score_features_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(score_features),\n",
    "    columns=score_features.columns,\n",
    "    index=score_features.index\n",
    ")\n",
    "\n",
    "# Add back stay_id\n",
    "score_features_processed = score_features_scaled.copy()\n",
    "score_features_processed['stay_id'] = scores_df['stay_id']\n",
    "\n",
    "print(\"\\nProcessed Clinical Scores Summary:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"\\nStatistics after processing:\")\n",
    "print(score_features_processed.describe().round(3))\n",
    "\n",
    "# Store transformers for later use on validation set\n",
    "score_transformers = {\n",
    "    'scaler': scaler\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b82c78da-903c-47a0-a10f-0e373da304e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comorbidity Summary:\n",
      "------------------------------\n",
      "\n",
      "Total rows: 13328\n",
      "\n",
      "Comorbidity Prevalence:\n",
      "mi: 2523 cases (18.9%)\n",
      "chf: 4180 cases (31.4%)\n",
      "copd: 3515 cases (26.4%)\n",
      "diabetes: 4185 cases (31.4%)\n",
      "liver_disease: 2059 cases (15.4%)\n",
      "cva: 2356 cases (17.7%)\n",
      "cancer: 1690 cases (12.7%)\n",
      "aids: 80 cases (0.6%)\n",
      "\n",
      "Missing Values (Count and Percentage):\n"
     ]
    }
   ],
   "source": [
    "# Convert stay_ids to regular Python integers\n",
    "stay_ids = tuple(int(x) for x in cohort_df['stay_id'].unique())\n",
    "\n",
    "# Revised comorbidity query with correct column names\n",
    "comorbidity_query = text(\"\"\"\n",
    "WITH cohort_admissions AS (\n",
    "    SELECT DISTINCT ie.subject_id, ie.hadm_id, ie.stay_id\n",
    "    FROM mimiciv_icu.icustays ie\n",
    "    WHERE ie.stay_id IN :stay_ids\n",
    ")\n",
    "SELECT \n",
    "    ca.stay_id,\n",
    "    COALESCE(ch.myocardial_infarct, 0) as mi,\n",
    "    COALESCE(ch.congestive_heart_failure, 0) as chf,\n",
    "    COALESCE(ch.chronic_pulmonary_disease, 0) as copd,\n",
    "    COALESCE(GREATEST(COALESCE(ch.diabetes_without_cc, 0), COALESCE(ch.diabetes_with_cc, 0)), 0) as diabetes,\n",
    "    COALESCE(ch.mild_liver_disease, 0) as liver_disease,\n",
    "    COALESCE(ch.cerebrovascular_disease, 0) as cva,\n",
    "    COALESCE(ch.malignant_cancer, 0) as cancer,\n",
    "    COALESCE(ch.aids, 0) as aids\n",
    "FROM cohort_admissions ca\n",
    "LEFT JOIN mimiciv_derived.charlson ch\n",
    "    ON ca.hadm_id = ch.hadm_id\n",
    "ORDER BY ca.stay_id;\n",
    "\"\"\").bindparams(stay_ids=stay_ids)\n",
    "\n",
    "# Execute query\n",
    "try:\n",
    "    comorbidity_df = pd.read_sql(comorbidity_query, engine)\n",
    "    \n",
    "    print(\"\\nComorbidity Summary:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"\\nTotal rows: {len(comorbidity_df)}\")\n",
    "    \n",
    "    # Prevalence of each comorbidity\n",
    "    print(\"\\nComorbidity Prevalence:\")\n",
    "    for col in comorbidity_df.columns:\n",
    "        if col != 'stay_id':\n",
    "            n_cases = comorbidity_df[col].sum()\n",
    "            pct = (n_cases / len(comorbidity_df) * 100)\n",
    "            print(f\"{col}: {n_cases} cases ({pct:.1f}%)\")\n",
    "    \n",
    "    # Missing value analysis\n",
    "    missing_counts = comorbidity_df.isnull().sum()\n",
    "    missing_percents = (comorbidity_df.isnull().sum() / len(comorbidity_df) * 100).round(2)\n",
    "    print(\"\\nMissing Values (Count and Percentage):\")\n",
    "    for col, count in missing_counts.items():\n",
    "        if count > 0:\n",
    "            print(f\"{col}: {count} ({missing_percents[col]}%)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in comorbidity extraction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "433d97e6-3fdb-4ccb-9381-34ae5ba6432e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRT Summary:\n",
      "------------------------------\n",
      "\n",
      "Total rows: 13328\n",
      "\n",
      "RRT cases: 1773 (13.3%)\n",
      "\n",
      "Missing Values (Count and Percentage):\n"
     ]
    }
   ],
   "source": [
    "# Convert stay_ids to regular Python integers\n",
    "stay_ids = tuple(int(x) for x in cohort_df['stay_id'].unique())\n",
    "\n",
    "# RRT query using correct derived table\n",
    "rrt_query = text(\"\"\"\n",
    "SELECT \n",
    "    s.stay_id,\n",
    "    COALESCE(MAX(CASE WHEN r.dialysis_active = 1 THEN 1 ELSE 0 END), 0) as rrt\n",
    "FROM (SELECT DISTINCT stay_id FROM mimiciv_icu.icustays WHERE stay_id IN :stay_ids) s\n",
    "LEFT JOIN mimiciv_derived.rrt r\n",
    "    ON s.stay_id = r.stay_id\n",
    "GROUP BY s.stay_id\n",
    "ORDER BY s.stay_id;\n",
    "\"\"\").bindparams(stay_ids=stay_ids)\n",
    "\n",
    "# Execute query\n",
    "try:\n",
    "    rrt_df = pd.read_sql(rrt_query, engine)\n",
    "    \n",
    "    print(\"\\nRRT Summary:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"\\nTotal rows: {len(rrt_df)}\")\n",
    "    \n",
    "    # RRT prevalence\n",
    "    n_cases = rrt_df['rrt'].sum()\n",
    "    pct = (n_cases / len(rrt_df) * 100)\n",
    "    print(f\"\\nRRT cases: {n_cases} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Missing value analysis\n",
    "    missing_counts = rrt_df.isnull().sum()\n",
    "    missing_percents = (rrt_df.isnull().sum() / len(rrt_df) * 100).round(2)\n",
    "    print(\"\\nMissing Values (Count and Percentage):\")\n",
    "    for col, count in missing_counts.items():\n",
    "        if count > 0:\n",
    "            print(f\"{col}: {count} ({missing_percents[col]}%)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in RRT extraction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be2f04cb-1caa-48e1-afe9-5856dbbcedd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tracheostomy Summary:\n",
      "------------------------------\n",
      "\n",
      "Total rows: 13328\n",
      "\n",
      "Tracheostomy cases: 797 (6.0%)\n",
      "\n",
      "Missing Values (Count and Percentage):\n"
     ]
    }
   ],
   "source": [
    "# Convert stay_ids to regular Python integers\n",
    "stay_ids = tuple(int(x) for x in cohort_df['stay_id'].unique())\n",
    "\n",
    "# Tracheostomy query - simply looking for presence of tracheostomy\n",
    "trach_query = text(\"\"\"\n",
    "SELECT \n",
    "    s.stay_id,\n",
    "    CASE \n",
    "        WHEN COUNT(v.ventilation_status) > 0 THEN 1\n",
    "        ELSE 0\n",
    "    END as has_tracheostomy\n",
    "FROM (SELECT DISTINCT stay_id FROM mimiciv_icu.icustays WHERE stay_id IN :stay_ids) s\n",
    "LEFT JOIN mimiciv_derived.ventilation v\n",
    "    ON s.stay_id = v.stay_id\n",
    "    AND v.ventilation_status = 'Tracheostomy'\n",
    "GROUP BY s.stay_id\n",
    "ORDER BY s.stay_id;\n",
    "\"\"\").bindparams(stay_ids=stay_ids)\n",
    "\n",
    "# Execute query\n",
    "try:\n",
    "    trach_df = pd.read_sql(trach_query, engine)\n",
    "    \n",
    "    print(\"\\nTracheostomy Summary:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"\\nTotal rows: {len(trach_df)}\")\n",
    "    \n",
    "    # Tracheostomy prevalence\n",
    "    n_cases = trach_df['has_tracheostomy'].sum()\n",
    "    pct = (n_cases / len(trach_df) * 100)\n",
    "    print(f\"\\nTracheostomy cases: {n_cases} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Missing value analysis\n",
    "    missing_counts = trach_df.isnull().sum()\n",
    "    missing_percents = (trach_df.isnull().sum() / len(trach_df) * 100).round(2)\n",
    "    print(\"\\nMissing Values (Count and Percentage):\")\n",
    "    for col, count in missing_counts.items():\n",
    "        if count > 0:\n",
    "            print(f\"{col}: {count} ({missing_percents[col]}%)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in tracheostomy extraction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00e030e8-a9a7-4969-a509-68660c29dd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ostomy Summary:\n",
      "------------------------------\n",
      "\n",
      "Total rows: 13328\n",
      "\n",
      "Ostomy cases: 550 (4.1%)\n",
      "\n",
      "Missing Values (Count and Percentage):\n"
     ]
    }
   ],
   "source": [
    "# Convert stay_ids to regular Python integers\n",
    "stay_ids = tuple(int(x) for x in cohort_df['stay_id'].unique())\n",
    "\n",
    "# Ostomy query focusing just on presence of ostomies\n",
    "ostomy_query = text(\"\"\"\n",
    "WITH ostomy_status AS (\n",
    "    SELECT \n",
    "        s.stay_id,\n",
    "        MAX(CASE WHEN \n",
    "            ce.itemid IN (\n",
    "                227458,  -- Ostomy\n",
    "                227637,  -- Ostomy Care\n",
    "                228341,  -- Ostomy Bag\n",
    "                228342,  -- Ostomy Output\n",
    "                228343,  -- Ostomy Type\n",
    "                228344,  -- Ostomy/RNWL\n",
    "                228345,  -- Ostomy/RNTL\n",
    "                228346,  -- Ostomy/RNY\n",
    "                228347   -- Ostomy/ROY\n",
    "            )\n",
    "        THEN 1 ELSE 0 END) as has_ostomy\n",
    "    FROM (SELECT DISTINCT stay_id FROM mimiciv_icu.icustays WHERE stay_id IN :stay_ids) s\n",
    "    LEFT JOIN mimiciv_icu.chartevents ce \n",
    "        ON s.stay_id = ce.stay_id\n",
    "    GROUP BY s.stay_id\n",
    ")\n",
    "SELECT \n",
    "    s.stay_id,\n",
    "    COALESCE(os.has_ostomy, 0) as has_ostomy\n",
    "FROM (SELECT DISTINCT stay_id FROM mimiciv_icu.icustays WHERE stay_id IN :stay_ids) s\n",
    "LEFT JOIN ostomy_status os \n",
    "    ON s.stay_id = os.stay_id\n",
    "ORDER BY s.stay_id;\n",
    "\"\"\").bindparams(stay_ids=stay_ids)\n",
    "\n",
    "# Execute query\n",
    "try:\n",
    "    ostomy_df = pd.read_sql(ostomy_query, engine)\n",
    "    \n",
    "    print(\"\\nOstomy Summary:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"\\nTotal rows: {len(ostomy_df)}\")\n",
    "    \n",
    "    # Ostomy prevalence\n",
    "    n_cases = ostomy_df['has_ostomy'].sum()\n",
    "    pct = (n_cases / len(ostomy_df) * 100)\n",
    "    print(f\"\\nOstomy cases: {n_cases} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Missing value analysis\n",
    "    missing_counts = ostomy_df.isnull().sum()\n",
    "    missing_percents = (ostomy_df.isnull().sum() / len(ostomy_df) * 100).round(2)\n",
    "    print(\"\\nMissing Values (Count and Percentage):\")\n",
    "    for col, count in missing_counts.items():\n",
    "        if count > 0:\n",
    "            print(f\"{col}: {count} ({missing_percents[col]}%)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in ostomy extraction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a9f7f38-74f0-41b3-86d5-1621fb2e01af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CVC Timing Summary:\n",
      "------------------------------\n",
      "\n",
      "Total rows: 13328\n",
      "\n",
      "LOS before CVC (hours) statistics:\n",
      "count    13328.000000\n",
      "mean        17.031826\n",
      "std         37.703097\n",
      "min       -336.261667\n",
      "25%          0.936875\n",
      "50%          3.489306\n",
      "75%         14.437083\n",
      "max        772.566667\n",
      "Name: los_before_cvc, dtype: float64\n",
      "\n",
      "Missing Values (Count and Percentage):\n"
     ]
    }
   ],
   "source": [
    "# Convert stay_ids to regular Python integers\n",
    "stay_ids = tuple(int(x) for x in cohort_df['stay_id'].unique())\n",
    "\n",
    "# Query for LOS before CVC\n",
    "cvc_timing_query = text(\"\"\"\n",
    "SELECT \n",
    "    s.stay_id,\n",
    "    EXTRACT(EPOCH FROM (MIN(il.starttime) - icu.intime))/3600 as los_before_cvc\n",
    "FROM (SELECT DISTINCT stay_id FROM mimiciv_icu.icustays WHERE stay_id IN :stay_ids) s\n",
    "LEFT JOIN mimiciv_icu.icustays icu\n",
    "    ON s.stay_id = icu.stay_id\n",
    "LEFT JOIN mimiciv_derived.invasive_line il\n",
    "    ON s.stay_id = il.stay_id\n",
    "    AND il.line_type IN ('PICC', 'Multi Lumen', 'Dialysis', 'Triple Introducer',\n",
    "                        'Pre-Sep', 'Hickman', 'Portacath', 'Cordis/Introducer',\n",
    "                        'Continuous Cardiac Output PA', 'PA')\n",
    "GROUP BY s.stay_id, icu.intime\n",
    "ORDER BY s.stay_id;\n",
    "\"\"\").bindparams(stay_ids=stay_ids)\n",
    "\n",
    "# Execute query\n",
    "try:\n",
    "    cvc_timing_df = pd.read_sql(cvc_timing_query, engine)\n",
    "    \n",
    "    print(\"\\nCVC Timing Summary:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"\\nTotal rows: {len(cvc_timing_df)}\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\nLOS before CVC (hours) statistics:\")\n",
    "    print(cvc_timing_df['los_before_cvc'].describe())\n",
    "    \n",
    "    # Missing value analysis\n",
    "    missing_counts = cvc_timing_df.isnull().sum()\n",
    "    missing_percents = (cvc_timing_df.isnull().sum() / len(cvc_timing_df) * 100).round(2)\n",
    "    print(\"\\nMissing Values (Count and Percentage):\")\n",
    "    for col, count in missing_counts.items():\n",
    "        if count > 0:\n",
    "            print(f\"{col}: {count} ({missing_percents[col]}%)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in CVC timing extraction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "451cceb4-bf23-4c77-acef-f14c3c049f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed CVC Timing Summary:\n",
      "------------------------------\n",
      "\n",
      "Original LOS before CVC (hours) statistics:\n",
      "count    13328.000000\n",
      "mean        17.117585\n",
      "std         37.432228\n",
      "min          0.000000\n",
      "25%          0.936875\n",
      "50%          3.489306\n",
      "75%         14.437083\n",
      "max        772.566667\n",
      "Name: los_before_cvc, dtype: float64\n",
      "\n",
      "Scaled LOS before CVC statistics:\n",
      "count    13328.000000\n",
      "mean         0.022157\n",
      "std          0.048452\n",
      "min          0.000000\n",
      "25%          0.001213\n",
      "50%          0.004517\n",
      "75%          0.018687\n",
      "max          1.000000\n",
      "Name: los_before_cvc_scaled, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Clean and scale the CVC timing data\n",
    "cvc_timing_processed = cvc_timing_df.copy()\n",
    "\n",
    "# 1. Handle negative values - set them to 0 since they likely represent lines present on admission\n",
    "cvc_timing_processed['los_before_cvc'] = cvc_timing_processed['los_before_cvc'].clip(lower=0)\n",
    "\n",
    "# 2. Scale the data using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "cvc_timing_processed['los_before_cvc_scaled'] = scaler.fit_transform(\n",
    "    cvc_timing_processed[['los_before_cvc']]\n",
    ")\n",
    "\n",
    "# Show results\n",
    "print(\"\\nProcessed CVC Timing Summary:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"\\nOriginal LOS before CVC (hours) statistics:\")\n",
    "print(cvc_timing_processed['los_before_cvc'].describe())\n",
    "print(\"\\nScaled LOS before CVC statistics:\")\n",
    "print(cvc_timing_processed['los_before_cvc_scaled'].describe())\n",
    "\n",
    "# Store transformer for validation set\n",
    "cvc_transformers = {\n",
    "    'scaler': scaler\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28720278-b9e2-4b1d-b810-c13fbeba8388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multiple Lines Summary:\n",
      "------------------------------\n",
      "\n",
      "Total rows: 13328\n",
      "\n",
      "Multiple lines cases: 5804 (43.5%)\n",
      "\n",
      "Line Type Counts:\n",
      "n_line_types\n",
      "1    8307\n",
      "2    3739\n",
      "3    1018\n",
      "4     207\n",
      "5      48\n",
      "6       8\n",
      "7       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing Values (Count and Percentage):\n"
     ]
    }
   ],
   "source": [
    "# Convert stay_ids to regular Python integers\n",
    "stay_ids = tuple(int(x) for x in cohort_df['stay_id'].unique())\n",
    "\n",
    "# Query for multiple lines - simplified version\n",
    "lines_query = text(\"\"\"\n",
    "WITH line_counts AS (\n",
    "    SELECT \n",
    "        s.stay_id,\n",
    "        COUNT(DISTINCT il.line_type) as unique_line_types,\n",
    "        CASE WHEN COUNT(*) > 1 THEN 1 ELSE 0 END as multiple_lines\n",
    "    FROM (SELECT DISTINCT stay_id FROM mimiciv_icu.icustays WHERE stay_id IN :stay_ids) s\n",
    "    LEFT JOIN mimiciv_derived.invasive_line il\n",
    "        ON s.stay_id = il.stay_id\n",
    "    WHERE il.line_type IN (\n",
    "        'PICC', \n",
    "        'Multi Lumen', \n",
    "        'Dialysis', \n",
    "        'Triple Introducer',\n",
    "        'Pre-Sep', \n",
    "        'Hickman', \n",
    "        'Portacath', \n",
    "        'Cordis/Introducer',\n",
    "        'Continuous Cardiac Output PA', \n",
    "        'PA'\n",
    "    )\n",
    "    GROUP BY s.stay_id\n",
    ")\n",
    "SELECT \n",
    "    s.stay_id,\n",
    "    COALESCE(lc.multiple_lines, 0) as multiple_lines,\n",
    "    COALESCE(lc.unique_line_types, 0) as n_line_types\n",
    "FROM (SELECT DISTINCT stay_id FROM mimiciv_icu.icustays WHERE stay_id IN :stay_ids) s\n",
    "LEFT JOIN line_counts lc\n",
    "    ON s.stay_id = lc.stay_id\n",
    "ORDER BY s.stay_id;\n",
    "\"\"\").bindparams(stay_ids=stay_ids)\n",
    "\n",
    "# Execute query\n",
    "try:\n",
    "    lines_df = pd.read_sql(lines_query, engine)\n",
    "    \n",
    "    print(\"\\nMultiple Lines Summary:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"\\nTotal rows: {len(lines_df)}\")\n",
    "    \n",
    "    # Multiple lines prevalence\n",
    "    n_cases = lines_df['multiple_lines'].sum()\n",
    "    pct = (n_cases / len(lines_df) * 100)\n",
    "    print(f\"\\nMultiple lines cases: {n_cases} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Line type counts statistics\n",
    "    print(\"\\nLine Type Counts:\")\n",
    "    print(lines_df['n_line_types'].value_counts().sort_index())\n",
    "    \n",
    "    # Missing value analysis\n",
    "    missing_counts = lines_df.isnull().sum()\n",
    "    missing_percents = (lines_df.isnull().sum() / len(lines_df) * 100).round(2)\n",
    "    print(\"\\nMissing Values (Count and Percentage):\")\n",
    "    for col, count in missing_counts.items():\n",
    "        if count > 0:\n",
    "            print(f\"{col}: {count} ({missing_percents[col]}%)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in lines extraction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b439731-20c8-41c9-bda7-44587d1d95fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed Multiple Lines Summary:\n",
      "------------------------------\n",
      "\n",
      "Total rows: 13328\n",
      "\n",
      "Encoded columns:\n",
      "['stay_id', 'multiple_lines', 'n_line_types_1', 'n_line_types_2', 'n_line_types_3', 'n_line_types_4']\n",
      "\n",
      "Sample of first few rows:\n",
      "    stay_id  multiple_lines  n_line_types_1  n_line_types_2  n_line_types_3  \\\n",
      "0  30003598               1           False            True           False   \n",
      "1  30004530               0            True           False           False   \n",
      "2  30005000               1           False            True           False   \n",
      "3  30005362               0            True           False           False   \n",
      "4  30005707               1            True           False           False   \n",
      "\n",
      "   n_line_types_4  \n",
      "0           False  \n",
      "1           False  \n",
      "2           False  \n",
      "3           False  \n",
      "4           False  \n",
      "\n",
      "Distribution of binned categories:\n",
      "n_line_types_1: 8307 cases (62.3%)\n",
      "n_line_types_2: 3739 cases (28.1%)\n",
      "n_line_types_3: 1018 cases (7.6%)\n",
      "n_line_types_4: 264 cases (2.0%)\n"
     ]
    }
   ],
   "source": [
    "# First modify n_line_types to bin 4+ together\n",
    "lines_df['n_line_types_binned'] = lines_df['n_line_types'].apply(lambda x: min(x, 4))\n",
    "\n",
    "# Create dummy variables\n",
    "line_types_encoded = pd.get_dummies(\n",
    "    lines_df['n_line_types_binned'], \n",
    "    prefix='n_line_types'\n",
    ")\n",
    "\n",
    "# Add back stay_id and multiple_lines flag\n",
    "lines_processed = pd.concat([\n",
    "    lines_df[['stay_id', 'multiple_lines']], \n",
    "    line_types_encoded\n",
    "], axis=1)\n",
    "\n",
    "# Show results\n",
    "print(\"\\nProcessed Multiple Lines Summary:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"\\nTotal rows: {len(lines_processed)}\")\n",
    "\n",
    "print(\"\\nEncoded columns:\")\n",
    "print(list(lines_processed.columns))\n",
    "\n",
    "print(\"\\nSample of first few rows:\")\n",
    "print(lines_processed.head())\n",
    "\n",
    "print(\"\\nDistribution of binned categories:\")\n",
    "for col in line_types_encoded.columns:\n",
    "    count = line_types_encoded[col].sum()\n",
    "    pct = (count / len(line_types_encoded) * 100)\n",
    "    print(f\"{col}: {count} cases ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "068396fe-4f08-4362-ac83-8d09e1f2ef11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed Multiple Lines Summary (with 1/0):\n",
      "------------------------------\n",
      "\n",
      "Sample of first few rows:\n",
      "    stay_id  multiple_lines  n_line_types_1  n_line_types_2  n_line_types_3  \\\n",
      "0  30003598               1               0               1               0   \n",
      "1  30004530               0               1               0               0   \n",
      "2  30005000               1               0               1               0   \n",
      "3  30005362               0               1               0               0   \n",
      "4  30005707               1               1               0               0   \n",
      "\n",
      "   n_line_types_4  \n",
      "0               0  \n",
      "1               0  \n",
      "2               0  \n",
      "3               0  \n",
      "4               0  \n",
      "\n",
      "Distribution of binned categories:\n",
      "n_line_types_1: 8307 cases (62.3%)\n",
      "n_line_types_2: 3739 cases (28.1%)\n",
      "n_line_types_3: 1018 cases (7.6%)\n",
      "n_line_types_4: 264 cases (2.0%)\n"
     ]
    }
   ],
   "source": [
    "# Convert boolean to integers (True/False to 1/0)\n",
    "bool_columns = ['n_line_types_1', 'n_line_types_2', 'n_line_types_3', 'n_line_types_4']\n",
    "lines_processed[bool_columns] = lines_processed[bool_columns].astype(int)\n",
    "\n",
    "print(\"\\nProcessed Multiple Lines Summary (with 1/0):\")\n",
    "print(\"-\" * 30)\n",
    "print(\"\\nSample of first few rows:\")\n",
    "print(lines_processed.head())\n",
    "\n",
    "print(\"\\nDistribution of binned categories:\")\n",
    "for col in bool_columns:\n",
    "    count = lines_processed[col].sum()\n",
    "    pct = (count / len(lines_processed) * 100)\n",
    "    print(f\"{col}: {count} cases ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14a69a53-eef7-4ac6-80e8-41203453c4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged Features Summary:\n",
      "------------------------------\n",
      "\n",
      "Total rows: 13328\n",
      "Total features: 65\n",
      "\n",
      "Missing Values Summary:\n",
      "No missing values found\n",
      "\n",
      "Sample of merged features (first 5 columns):\n",
      "    stay_id gender        age               ethnicity  wbc_mean\n",
      "0  37510196      F  77.018296  BLACK/AFRICAN AMERICAN  0.047413\n",
      "1  33987268      F  81.280232                   WHITE  0.078656\n",
      "2  32128372      F  75.149738  BLACK/AFRICAN AMERICAN  0.013240\n",
      "3  32824762      M  62.365832              PORTUGUESE  0.052280\n",
      "4  34100191      M  47.149517      BLACK/CAPE VERDEAN  0.046974\n"
     ]
    }
   ],
   "source": [
    "# List of DataFrames we have:\n",
    "# 1. Demographic features (demo_df)\n",
    "# 2. Lab values (lab_features_processed) - scaled\n",
    "# 3. Vital signs (vital_features_processed) - scaled\n",
    "# 4. Clinical scores (score_features_processed) - scaled\n",
    "# 5. Comorbidities (comorbidity_df)\n",
    "# 6. RRT (rrt_df)\n",
    "# 7. LOS before CVC (cvc_timing_processed) - scaled\n",
    "# 8. Tracheostomy (trach_df)\n",
    "# 9. Ostomy (ostomy_df)\n",
    "# 10. Multiple lines (lines_processed)\n",
    "\n",
    "# Merge all features using stay_id as key\n",
    "merged_features = pd.merge(demo_df, lab_features_processed, on='stay_id', how='left')\n",
    "merged_features = pd.merge(merged_features, vital_features_processed, on='stay_id', how='left')\n",
    "merged_features = pd.merge(merged_features, score_features_processed, on='stay_id', how='left')\n",
    "merged_features = pd.merge(merged_features, comorbidity_df, on='stay_id', how='left')\n",
    "merged_features = pd.merge(merged_features, rrt_df, on='stay_id', how='left')\n",
    "merged_features = pd.merge(merged_features, cvc_timing_processed, on='stay_id', how='left')\n",
    "merged_features = pd.merge(merged_features, trach_df, on='stay_id', how='left')\n",
    "merged_features = pd.merge(merged_features, ostomy_df, on='stay_id', how='left')\n",
    "merged_features = pd.merge(merged_features, lines_processed, on='stay_id', how='left')\n",
    "\n",
    "# Check merged dataset\n",
    "print(\"\\nMerged Features Summary:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"\\nTotal rows: {len(merged_features)}\")\n",
    "print(f\"Total features: {len(merged_features.columns)}\")\n",
    "\n",
    "# Check for any missing values after merge\n",
    "print(\"\\nMissing Values Summary:\")\n",
    "missing = merged_features.isnull().sum()[merged_features.isnull().sum() > 0]\n",
    "if len(missing) > 0:\n",
    "    print(missing)\n",
    "else:\n",
    "    print(\"No missing values found\")\n",
    "\n",
    "# Show sample of first few rows\n",
    "print(\"\\nSample of merged features (first 5 columns):\")\n",
    "print(merged_features.iloc[:5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adc36000-d52e-44a7-9b2d-066a6e755924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Verification Summary:\n",
      "------------------------------\n",
      "\n",
      "Binary Features Check:\n",
      "multiple_lines: unique values = [0, 1]\n",
      "n_line_types_1: unique values = [0, 1]\n",
      "n_line_types_2: unique values = [0, 1]\n",
      "n_line_types_3: unique values = [0, 1]\n",
      "n_line_types_4: unique values = [0, 1]\n",
      "has_tracheostomy: unique values = [0, 1]\n",
      "has_ostomy: unique values = [0, 1]\n",
      "rrt: unique values = [0, 1]\n",
      "\n",
      "Scaled Features Check (should be between 0-1):\n",
      "wbc_mean: range = [0.000, 1.000]\n",
      "platelet_mean: range = [0.000, 1.000]\n",
      "hemoglobin_mean: range = [0.000, 1.000]\n",
      "los_before_cvc_scaled: range = [0.000, 1.000]\n",
      "\n",
      "Categorical Features Unique Values:\n",
      "gender: 2 unique values\n",
      "ethnicity: 33 unique values\n",
      "\n",
      "Duplicate Column Check:\n",
      "No duplicate columns found\n",
      "\n",
      "Dataset saved successfully as 'clabsi_features_final.csv'\n",
      "Dataset saved successfully as 'clabsi_features_final.pkl'\n",
      "\n",
      "Full list of features:\n",
      "1. stay_id\n",
      "2. gender\n",
      "3. age\n",
      "4. ethnicity\n",
      "5. wbc_mean\n",
      "6. wbc_min\n",
      "7. wbc_max\n",
      "8. platelet_mean\n",
      "9. platelet_min\n",
      "10. platelet_max\n",
      "11. hemoglobin_mean\n",
      "12. hemoglobin_min\n",
      "13. hemoglobin_max\n",
      "14. aniongap_mean\n",
      "15. bicarbonate_mean\n",
      "16. creatinine_mean\n",
      "17. chloride_mean\n",
      "18. glucose_mean\n",
      "19. sodium_mean\n",
      "20. potassium_mean\n",
      "21. inr_mean\n",
      "22. pt_mean\n",
      "23. ptt_mean\n",
      "24. heart_rate_mean\n",
      "25. heart_rate_min\n",
      "26. heart_rate_max\n",
      "27. sbp_mean\n",
      "28. sbp_min\n",
      "29. sbp_max\n",
      "30. dbp_mean\n",
      "31. dbp_min\n",
      "32. dbp_max\n",
      "33. mbp_mean\n",
      "34. mbp_min\n",
      "35. mbp_max\n",
      "36. resp_rate_mean\n",
      "37. resp_rate_min\n",
      "38. resp_rate_max\n",
      "39. temperature_mean\n",
      "40. temperature_min\n",
      "41. temperature_max\n",
      "42. spo2_mean\n",
      "43. spo2_min\n",
      "44. spo2_max\n",
      "45. sofa_score\n",
      "46. apsiii_score\n",
      "47. sapsii_score\n",
      "48. mi\n",
      "49. chf\n",
      "50. copd\n",
      "51. diabetes\n",
      "52. liver_disease\n",
      "53. cva\n",
      "54. cancer\n",
      "55. aids\n",
      "56. rrt\n",
      "57. los_before_cvc\n",
      "58. los_before_cvc_scaled\n",
      "59. has_tracheostomy\n",
      "60. has_ostomy\n",
      "61. multiple_lines\n",
      "62. n_line_types_1\n",
      "63. n_line_types_2\n",
      "64. n_line_types_3\n",
      "65. n_line_types_4\n"
     ]
    }
   ],
   "source": [
    "# 1. First let's categorize our columns\n",
    "binary_features = [\n",
    "    'multiple_lines', 'n_line_types_1', 'n_line_types_2', \n",
    "    'n_line_types_3', 'n_line_types_4', 'has_tracheostomy', \n",
    "    'has_ostomy', 'rrt'\n",
    "]\n",
    "\n",
    "scaled_features = [\n",
    "    'wbc_mean', 'platelet_mean', 'hemoglobin_mean', \n",
    "    'los_before_cvc_scaled',\n",
    "    # Add other lab and vital features here\n",
    "]\n",
    "\n",
    "categorical_features = ['gender', 'ethnicity']\n",
    "\n",
    "# Verification checks\n",
    "print(\"Feature Verification Summary:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Check binary features\n",
    "print(\"\\nBinary Features Check:\")\n",
    "for col in binary_features:\n",
    "    unique_vals = sorted(merged_features[col].unique())\n",
    "    print(f\"{col}: unique values = {unique_vals}\")\n",
    "\n",
    "# Check scaled features\n",
    "print(\"\\nScaled Features Check (should be between 0-1):\")\n",
    "for col in scaled_features:\n",
    "    min_val = merged_features[col].min()\n",
    "    max_val = merged_features[col].max()\n",
    "    print(f\"{col}: range = [{min_val:.3f}, {max_val:.3f}]\")\n",
    "\n",
    "# Check categorical features\n",
    "print(\"\\nCategorical Features Unique Values:\")\n",
    "for col in categorical_features:\n",
    "    n_unique = merged_features[col].nunique()\n",
    "    print(f\"{col}: {n_unique} unique values\")\n",
    "\n",
    "# Check for duplicate column names\n",
    "print(\"\\nDuplicate Column Check:\")\n",
    "duplicates = merged_features.columns[merged_features.columns.duplicated()].tolist()\n",
    "if duplicates:\n",
    "    print(f\"Found duplicate columns: {duplicates}\")\n",
    "else:\n",
    "    print(\"No duplicate columns found\")\n",
    "\n",
    "# Save the final dataset\n",
    "try:\n",
    "    # Save as CSV\n",
    "    merged_features.to_csv('clabsi_features_final.csv', index=False)\n",
    "    print(\"\\nDataset saved successfully as 'clabsi_features_final.csv'\")\n",
    "    \n",
    "    # Also save as pickle to preserve data types\n",
    "    merged_features.to_pickle('clabsi_features_final.pkl')\n",
    "    print(\"Dataset saved successfully as 'clabsi_features_final.pkl'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving dataset: {e}\")\n",
    "\n",
    "# Print column names for reference\n",
    "print(\"\\nFull list of features:\")\n",
    "for i, col in enumerate(merged_features.columns, 1):\n",
    "    print(f\"{i}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a72b5802-0e87-4759-b1e5-003b201db6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features After Processing:\n",
      "------------------------------\n",
      "\n",
      "Total features before: 65\n",
      "Total features after: 75\n",
      "\n",
      "Sample of first few rows (first 5 columns):\n",
      "    stay_id  gender        age  wbc_mean  platelet_mean\n",
      "0  37510196       0  77.018296  0.047413       0.070251\n",
      "1  33987268       0  81.280232  0.078656       0.119488\n",
      "2  32128372       0  75.149738  0.013240       0.034819\n",
      "3  32824762       1  62.365832  0.052280       0.088657\n",
      "4  34100191       1  47.149517  0.046974       0.005645\n",
      "\n",
      "Final list of features:\n",
      "1. stay_id\n",
      "2. gender\n",
      "3. age\n",
      "4. wbc_mean\n",
      "5. platelet_mean\n",
      "6. hemoglobin_mean\n",
      "7. aniongap_mean\n",
      "8. bicarbonate_mean\n",
      "9. creatinine_mean\n",
      "10. chloride_mean\n",
      "11. glucose_mean\n",
      "12. sodium_mean\n",
      "13. potassium_mean\n",
      "14. inr_mean\n",
      "15. pt_mean\n",
      "16. ptt_mean\n",
      "17. heart_rate_mean\n",
      "18. sbp_mean\n",
      "19. dbp_mean\n",
      "20. mbp_mean\n",
      "21. resp_rate_mean\n",
      "22. temperature_mean\n",
      "23. spo2_mean\n",
      "24. sofa_score\n",
      "25. apsiii_score\n",
      "26. sapsii_score\n",
      "27. mi\n",
      "28. chf\n",
      "29. copd\n",
      "30. diabetes\n",
      "31. liver_disease\n",
      "32. cva\n",
      "33. cancer\n",
      "34. aids\n",
      "35. rrt\n",
      "36. los_before_cvc_scaled\n",
      "37. has_tracheostomy\n",
      "38. has_ostomy\n",
      "39. multiple_lines\n",
      "40. n_line_types_1\n",
      "41. n_line_types_2\n",
      "42. n_line_types_3\n",
      "43. n_line_types_4\n",
      "44. ethnicity_ASIAN\n",
      "45. ethnicity_ASIAN - ASIAN INDIAN\n",
      "46. ethnicity_ASIAN - CHINESE\n",
      "47. ethnicity_ASIAN - KOREAN\n",
      "48. ethnicity_ASIAN - SOUTH EAST ASIAN\n",
      "49. ethnicity_BLACK/AFRICAN\n",
      "50. ethnicity_BLACK/AFRICAN AMERICAN\n",
      "51. ethnicity_BLACK/CAPE VERDEAN\n",
      "52. ethnicity_BLACK/CARIBBEAN ISLAND\n",
      "53. ethnicity_HISPANIC OR LATINO\n",
      "54. ethnicity_HISPANIC/LATINO - CENTRAL AMERICAN\n",
      "55. ethnicity_HISPANIC/LATINO - COLUMBIAN\n",
      "56. ethnicity_HISPANIC/LATINO - CUBAN\n",
      "57. ethnicity_HISPANIC/LATINO - DOMINICAN\n",
      "58. ethnicity_HISPANIC/LATINO - GUATEMALAN\n",
      "59. ethnicity_HISPANIC/LATINO - HONDURAN\n",
      "60. ethnicity_HISPANIC/LATINO - MEXICAN\n",
      "61. ethnicity_HISPANIC/LATINO - PUERTO RICAN\n",
      "62. ethnicity_HISPANIC/LATINO - SALVADORAN\n",
      "63. ethnicity_MULTIPLE RACE/ETHNICITY\n",
      "64. ethnicity_NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER\n",
      "65. ethnicity_OTHER\n",
      "66. ethnicity_PATIENT DECLINED TO ANSWER\n",
      "67. ethnicity_PORTUGUESE\n",
      "68. ethnicity_SOUTH AMERICAN\n",
      "69. ethnicity_UNABLE TO OBTAIN\n",
      "70. ethnicity_UNKNOWN\n",
      "71. ethnicity_WHITE\n",
      "72. ethnicity_WHITE - BRAZILIAN\n",
      "73. ethnicity_WHITE - EASTERN EUROPEAN\n",
      "74. ethnicity_WHITE - OTHER EUROPEAN\n",
      "75. ethnicity_WHITE - RUSSIAN\n"
     ]
    }
   ],
   "source": [
    "# 1. First identify redundant features to remove\n",
    "redundant_features = [\n",
    "    # Remove raw LOS since we have scaled\n",
    "    'los_before_cvc',\n",
    "    \n",
    "    # Remove min/max keeping only means for labs\n",
    "    'wbc_min', 'wbc_max',\n",
    "    'platelet_min', 'platelet_max',\n",
    "    'hemoglobin_min', 'hemoglobin_max',\n",
    "    \n",
    "    # Remove min/max keeping only means for vitals\n",
    "    'heart_rate_min', 'heart_rate_max',\n",
    "    'sbp_min', 'sbp_max',\n",
    "    'dbp_min', 'dbp_max',\n",
    "    'mbp_min', 'mbp_max',\n",
    "    'resp_rate_min', 'resp_rate_max',\n",
    "    'temperature_min', 'temperature_max',\n",
    "    'spo2_min', 'spo2_max'\n",
    "]\n",
    "\n",
    "# 2. Create clean dataset removing redundant features\n",
    "clean_features = merged_features.drop(columns=redundant_features)\n",
    "\n",
    "# 3. Encode categorical variables\n",
    "# Encode gender\n",
    "clean_features['gender'] = (clean_features['gender'] == 'M').astype(int)\n",
    "\n",
    "# One-hot encode ethnicity, dropping first category to avoid multicollinearity\n",
    "ethnicity_encoded = pd.get_dummies(clean_features['ethnicity'], prefix='ethnicity', drop_first=True)\n",
    "clean_features = pd.concat([clean_features.drop('ethnicity', axis=1), ethnicity_encoded], axis=1)\n",
    "\n",
    "# Show results\n",
    "print(\"Features After Processing:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"\\nTotal features before: {len(merged_features.columns)}\")\n",
    "print(f\"Total features after: {len(clean_features.columns)}\")\n",
    "\n",
    "print(\"\\nSample of first few rows (first 5 columns):\")\n",
    "print(clean_features.iloc[:5, :5])\n",
    "\n",
    "print(\"\\nFinal list of features:\")\n",
    "for i, col in enumerate(clean_features.columns, 1):\n",
    "    print(f\"{i}. {col}\")\n",
    "\n",
    "# Save processed dataset\n",
    "clean_features.to_csv('clabsi_features_processed.csv', index=False)\n",
    "clean_features.to_pickle('clabsi_features_processed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "023a0e0f-a05b-40b6-a46c-9ec64f64cdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features After Removing Ethnicity:\n",
      "------------------------------\n",
      "\n",
      "Total features before: 75\n",
      "Total features after: 43\n",
      "\n",
      "Final list of features:\n",
      "1. stay_id\n",
      "2. gender\n",
      "3. age\n",
      "4. wbc_mean\n",
      "5. platelet_mean\n",
      "6. hemoglobin_mean\n",
      "7. aniongap_mean\n",
      "8. bicarbonate_mean\n",
      "9. creatinine_mean\n",
      "10. chloride_mean\n",
      "11. glucose_mean\n",
      "12. sodium_mean\n",
      "13. potassium_mean\n",
      "14. inr_mean\n",
      "15. pt_mean\n",
      "16. ptt_mean\n",
      "17. heart_rate_mean\n",
      "18. sbp_mean\n",
      "19. dbp_mean\n",
      "20. mbp_mean\n",
      "21. resp_rate_mean\n",
      "22. temperature_mean\n",
      "23. spo2_mean\n",
      "24. sofa_score\n",
      "25. apsiii_score\n",
      "26. sapsii_score\n",
      "27. mi\n",
      "28. chf\n",
      "29. copd\n",
      "30. diabetes\n",
      "31. liver_disease\n",
      "32. cva\n",
      "33. cancer\n",
      "34. aids\n",
      "35. rrt\n",
      "36. los_before_cvc_scaled\n",
      "37. has_tracheostomy\n",
      "38. has_ostomy\n",
      "39. multiple_lines\n",
      "40. n_line_types_1\n",
      "41. n_line_types_2\n",
      "42. n_line_types_3\n",
      "43. n_line_types_4\n"
     ]
    }
   ],
   "source": [
    "# Get list of ethnicity columns\n",
    "ethnicity_columns = [col for col in clean_features.columns if col.startswith('ethnicity_')]\n",
    "\n",
    "# Remove ethnicity columns\n",
    "clean_features_no_eth = clean_features.drop(columns=ethnicity_columns)\n",
    "\n",
    "print(\"Features After Removing Ethnicity:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"\\nTotal features before: {len(clean_features.columns)}\")\n",
    "print(f\"Total features after: {len(clean_features_no_eth.columns)}\")\n",
    "\n",
    "print(\"\\nFinal list of features:\")\n",
    "for i, col in enumerate(clean_features_no_eth.columns, 1):\n",
    "    print(f\"{i}. {col}\")\n",
    "\n",
    "# Save updated dataset\n",
    "clean_features_no_eth.to_csv('clabsi_features_final_no_eth.csv', index=False)\n",
    "clean_features_no_eth.to_pickle('clabsi_features_final_no_eth.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e705d09-ff22-4004-af93-4964dd740866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Splits:\n",
      "------------------------------\n",
      "\n",
      "Training:\n",
      "Total: 9431\n",
      "CLABSI cases: 83\n",
      "30-day mortality: 2092\n",
      "\n",
      "Validation:\n",
      "Total: 3897\n",
      "CLABSI cases: 31\n",
      "30-day mortality: 912\n"
     ]
    }
   ],
   "source": [
    "# Verify splits from cohort_df\n",
    "print(\"Original Splits:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"\\nTraining:\")\n",
    "print(f\"Total: {len(train_df)}\")\n",
    "print(f\"CLABSI cases: {train_df['has_clabsi'].sum()}\")\n",
    "print(f\"30-day mortality: {train_df['mortality_30d'].sum()}\")\n",
    "\n",
    "print(\"\\nValidation:\")\n",
    "print(f\"Total: {len(val_df)}\")\n",
    "print(f\"CLABSI cases: {val_df['has_clabsi'].sum()}\")\n",
    "print(f\"30-day mortality: {val_df['mortality_30d'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "668af6cd-6bad-445b-b002-bf3a4429e1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Datasets:\n",
      "------------------------------\n",
      "\n",
      "Training set shape: (9431, 45)\n",
      "Validation set shape: (3897, 45)\n"
     ]
    }
   ],
   "source": [
    "# Merge features with original splits\n",
    "train_final = pd.merge(\n",
    "    train_df[['stay_id', 'has_clabsi', 'mortality_30d']], \n",
    "    clean_features_no_eth,\n",
    "    on='stay_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "val_final = pd.merge(\n",
    "    val_df[['stay_id', 'has_clabsi', 'mortality_30d']], \n",
    "    clean_features_no_eth,\n",
    "    on='stay_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Datasets:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"\\nTraining set shape: {train_final.shape}\")\n",
    "print(f\"Validation set shape: {val_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424d1c12-d8c1-4002-abb7-bbba867bbbb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
