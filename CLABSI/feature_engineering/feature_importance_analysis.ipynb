{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2aaef3-0bc9-4a09-910e-7698e1d9758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text, bindparam\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from clabsi_feature_engineering import (\n",
    "    connect_db, \n",
    "    get_clabsi_cohort,\n",
    "    get_control_cohort,\n",
    "    CLABSIFeatureProcessor\n",
    ")\n",
    "\n",
    "engine = connect_db()\n",
    "if engine is None:\n",
    "    raise Exception(\"Database connection failed. Please check your connection parameters.\")\n",
    "\n",
    "print(\"Getting cohorts...\")\n",
    "clabsi_cohort = get_clabsi_cohort(engine)\n",
    "control_cohort = get_control_cohort(engine, clabsi_cohort)\n",
    "\n",
    "\n",
    "if clabsi_cohort.empty:\n",
    "    raise Exception(\"CLABSI cohort is empty!\")\n",
    "if control_cohort.empty:\n",
    "    raise Exception(\"Control cohort is empty!\")\n",
    "\n",
    "\n",
    "processor = CLABSIFeatureProcessor(engine)\n",
    "\n",
    "\n",
    "print(\"\\nProcessing CLABSI cohort...\")\n",
    "clabsi_features = processor.get_batch_features(clabsi_cohort)\n",
    "if clabsi_features is not None:\n",
    "    clabsi_features['is_clabsi'] = 1\n",
    "else:\n",
    "    raise Exception(\"Failed to process CLABSI features!\")\n",
    "\n",
    "print(\"\\nProcessing control cohort...\")\n",
    "control_features = processor.get_batch_features(control_cohort)\n",
    "if control_features is not None:\n",
    "    control_features['is_clabsi'] = 0\n",
    "else:\n",
    "    raise Exception(\"Failed to process control features!\")\n",
    "\n",
    "\n",
    "combined_df = pd.concat([clabsi_features, control_features], axis=0).reset_index(drop=True)\n",
    "print(f\"\\nInitial combined dataset shape: {combined_df.shape}\")\n",
    "\n",
    "\n",
    "print(combined_df.info())\n",
    "print(combined_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f9cc08-0aa4-47ad-8ad8-4e16115cbcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import traceback\n",
    "\n",
    "\n",
    "def load_analysis_data():\n",
    "    \"\"\"\n",
    "    Load analysis data by:\n",
    "      - Connecting to the MIMIC IV database.\n",
    "      - Retrieving the CLABSI and control cohorts.\n",
    "      - Processing features for both cohorts using CLABSIFeatureProcessor.\n",
    "      - Combining the two datasets.\n",
    "      \n",
    "    Returns:\n",
    "      combined_df (pd.DataFrame): Combined feature dataset.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Connect to database\n",
    "        engine = connect_db()\n",
    "        if not engine:\n",
    "            print(\"Database connection failed.\")\n",
    "            return None\n",
    "        \n",
    "        print(\"Getting cohorts...\")\n",
    "        clabsi_cohort = get_clabsi_cohort(engine)\n",
    "        control_cohort = get_control_cohort(engine, clabsi_cohort)\n",
    "        \n",
    "        # Initialize feature processor\n",
    "        processor = CLABSIFeatureProcessor(engine)\n",
    "        \n",
    "        # Process CLABSI cohort\n",
    "        print(\"Processing CLABSI cohort...\")\n",
    "        clabsi_features = processor.get_batch_features(clabsi_cohort)\n",
    "        if clabsi_features is not None:\n",
    "            clabsi_features['is_clabsi'] = 1\n",
    "        else:\n",
    "            print(\"Error processing CLABSI features.\")\n",
    "            return None\n",
    "        \n",
    "        # Process control cohort\n",
    "        print(\"Processing control cohort...\")\n",
    "        control_features = processor.get_batch_features(control_cohort)\n",
    "        if control_features is not None:\n",
    "            control_features['is_clabsi'] = 0\n",
    "        else:\n",
    "            print(\"Error processing control features.\")\n",
    "            return None\n",
    "        \n",
    "        # Combine datasets\n",
    "        combined_df = pd.concat([clabsi_features, control_features], axis=0)\n",
    "        print(f\"\\nFinal dataset shape: {combined_df.shape}\")\n",
    "        return combined_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "def clean_combined_data(df):\n",
    "    \"\"\"\n",
    "    Clean and validate the combined dataset.\n",
    "    \n",
    "    Steps:\n",
    "      1. Remove duplicate rows based on 'stay_id'.\n",
    "      2. Report missing values in critical columns.\n",
    "      3. For both CLABSI cases and controls, impute negative \n",
    "         'days_since_last_dressing_change' values with 0.\n",
    "      4. Validate that key numeric values fall within reasonable ranges.\n",
    "      5. Report the number of rows removed and the final CLABSI distribution.\n",
    "    \"\"\"\n",
    "    import warnings\n",
    "\n",
    "    print(\"\\nStarting data cleaning process...\")\n",
    "    df_clean = df.copy()\n",
    "    original_shape = df_clean.shape\n",
    "\n",
    "    # Remove duplicates based on 'stay_id'\n",
    "    duplicates = df_clean.duplicated(subset=['stay_id'], keep='first')\n",
    "    if duplicates.any():\n",
    "        num_duplicates = duplicates.sum()\n",
    "        print(f\"Found {num_duplicates} duplicate stay_ids. Removing duplicates.\")\n",
    "        df_clean = df_clean.drop_duplicates(subset=['stay_id'], keep='first')\n",
    "    else:\n",
    "        print(\"No duplicate stay_ids found.\")\n",
    "\n",
    "    # Define critical columns for cleaning diagnostics\n",
    "    critical_columns = [\n",
    "        'total_baths', 'days_since_last_dressing_change',\n",
    "        'admission_age', 'plt_mean', 'wbc_mean'\n",
    "    ]\n",
    "    missing_overall = df_clean[critical_columns].isnull().sum()\n",
    "    print(\"\\nMissing values in critical columns (overall):\")\n",
    "    print(missing_overall[missing_overall > 0])\n",
    "    \n",
    "    # Handle negative values in 'days_since_last_dressing_change'\n",
    "    if 'is_clabsi' in df_clean.columns:\n",
    "        cases = df_clean[df_clean['is_clabsi'] == 1].copy()\n",
    "        controls = df_clean[df_clean['is_clabsi'] == 0].copy()\n",
    "        \n",
    "        neg_cases = (cases['days_since_last_dressing_change'] < 0).sum()\n",
    "        neg_controls = (controls['days_since_last_dressing_change'] < 0).sum()\n",
    "        if neg_cases > 0:\n",
    "            print(f\"Found {neg_cases} negative 'days_since_last_dressing_change' values in cases. Imputing with 0.\")\n",
    "            cases.loc[cases['days_since_last_dressing_change'] < 0, 'days_since_last_dressing_change'] = 0\n",
    "        if neg_controls > 0:\n",
    "            print(f\"Found {neg_controls} negative 'days_since_last_dressing_change' values in controls. Imputing with 0.\")\n",
    "            controls.loc[controls['days_since_last_dressing_change'] < 0, 'days_since_last_dressing_change'] = 0\n",
    "        \n",
    "        df_clean = pd.concat([cases, controls], axis=0)\n",
    "    else:\n",
    "        neg_count = (df_clean['days_since_last_dressing_change'] < 0).sum()\n",
    "        if neg_count > 0:\n",
    "            print(f\"Found {neg_count} negative 'days_since_last_dressing_change' values. Imputing with 0.\")\n",
    "            df_clean.loc[df_clean['days_since_last_dressing_change'] < 0, 'days_since_last_dressing_change'] = 0\n",
    "\n",
    "\n",
    "    print(\"\\nValidating value ranges...\")\n",
    "    df_clean = df_clean[(df_clean['admission_age'] > 0) & (df_clean['admission_age'] < 120)]\n",
    "    df_clean = df_clean[(df_clean['plt_mean'] > 0) & (df_clean['wbc_mean'] > 0)]\n",
    "    \n",
    "    final_shape = df_clean.shape\n",
    "    rows_removed = original_shape[0] - final_shape[0]\n",
    "    print(f\"\\nOriginal shape: {original_shape}\")\n",
    "    print(f\"Cleaned shape: {final_shape}\")\n",
    "    print(f\"Removed {rows_removed} rows ({rows_removed/original_shape[0]:.1%}) during cleaning\")\n",
    "    \n",
    "    if 'is_clabsi' in df_clean.columns:\n",
    "        clabsi_dist = df_clean['is_clabsi'].value_counts(normalize=True)\n",
    "        print(\"\\nCLABSI distribution after cleaning:\")\n",
    "        print(clabsi_dist)\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def prepare_features(combined_df):\n",
    "    \"\"\"\n",
    "    Prepare features for importance analysis from the cleaned dataset.\n",
    "    \n",
    "    Returns:\n",
    "      X (pd.DataFrame): Feature matrix.\n",
    "      y (pd.Series): Target variable.\n",
    "      feature_categories (dict): Dictionary of feature categories.\n",
    "    \"\"\"\n",
    "    # Define feature categories (note: 'total_baths' has been removed)\n",
    "    feature_categories = {\n",
    "        'line_characteristics': [\n",
    "            'line_duration_days', 'line_type', 'line_site', \n",
    "            'site_category', 'risk_category'\n",
    "        ],\n",
    "        'patient_factors': [\n",
    "            'admission_age', 'gender', 'has_diabetes', 'has_cancer',\n",
    "            'has_liver', 'has_chf', 'has_cva'\n",
    "        ],\n",
    "        'clinical_care': [\n",
    "            'days_since_last_dressing_change', 'chg_adherence_ratio'\n",
    "        ],\n",
    "        'lab_values': [\n",
    "            'wbc_mean', 'plt_mean', 'creat_mean', 'inr_mean',\n",
    "            'pt_mean', 'ptt_mean'\n",
    "        ],\n",
    "        'severity_scores': [\n",
    "            'sofa_score', 'apsiii', 'sapsii'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    \n",
    "    y = combined_df['is_clabsi']\n",
    "    \n",
    "   \n",
    "    feature_cols = []\n",
    "    for cat in feature_categories.values():\n",
    "        feature_cols.extend([col for col in cat if col in combined_df.columns])\n",
    "    \n",
    "  \n",
    "    X = combined_df[feature_cols].copy()\n",
    "    \n",
    "   \n",
    "    categorical_features = X.select_dtypes(include=['object']).columns\n",
    "    X = pd.get_dummies(X, columns=categorical_features)\n",
    "    \n",
    "   \n",
    "    X = X.fillna(X.mean())\n",
    "    \n",
    "    return X, y, feature_categories\n",
    "\n",
    "\n",
    "def calculate_feature_importance(X, y):\n",
    "    \"\"\"\n",
    "    Calculate feature importance using:\n",
    "      1. XGBoost built-in feature importance.\n",
    "      2. Mutual Information.\n",
    "      3. SHAP values.\n",
    "      \n",
    "    Returns:\n",
    "      importance_df (pd.DataFrame): DataFrame with composite importance scores.\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "    import xgboost as xgb\n",
    "    import shap\n",
    "    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "   \n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        random_state=42,\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1\n",
    "    )\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    xgb_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'xgb_importance': xgb_model.feature_importances_\n",
    "    })\n",
    "    \n",
    "    \n",
    "    mi_selector = SelectKBest(score_func=mutual_info_classif, k='all')\n",
    "    mi_selector.fit(X_train, y_train)\n",
    "    mi_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'mutual_info_score': mi_selector.scores_\n",
    "    })\n",
    "    \n",
    "   \n",
    "    explainer = shap.TreeExplainer(xgb_model)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    shap_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'shap_importance': np.abs(shap_values).mean(axis=0)\n",
    "    })\n",
    "    \n",
    "   \n",
    "    importance_df = xgb_importance.merge(mi_importance, on='feature')\n",
    "    importance_df = importance_df.merge(shap_importance, on='feature')\n",
    "    \n",
    "   \n",
    "    for col in ['xgb_importance', 'mutual_info_score', 'shap_importance']:\n",
    "        importance_df[col] = importance_df[col] / importance_df[col].max()\n",
    "    \n",
    "    \n",
    "    importance_df['composite_score'] = importance_df[['xgb_importance', 'mutual_info_score', 'shap_importance']].mean(axis=1)\n",
    "    \n",
    "    return importance_df.sort_values('composite_score', ascending=False)\n",
    "\n",
    "\n",
    "def diagnostic_plots(df, group_col='is_clabsi', critical_columns=None, title_prefix=\"Before Cleaning\"):\n",
    "    \"\"\"\n",
    "    Plot summary statistics and histograms for key critical columns,\n",
    "    optionally grouped by a specified column.\n",
    "    \"\"\"\n",
    "    if critical_columns is None:\n",
    "        critical_columns = ['total_baths', 'days_since_last_dressing_change', 'admission_age', 'plt_mean', 'wbc_mean']\n",
    "    \n",
    "    if group_col in df.columns:\n",
    "        groups = df.groupby(group_col)\n",
    "    else:\n",
    "        groups = [(\"All\", df)]\n",
    "    \n",
    "    for group_name, group_df in groups:\n",
    "        print(f\"\\nSummary statistics for group '{group_name}' {title_prefix}:\")\n",
    "        print(group_df[critical_columns].describe())\n",
    "        print(f\"\\nMissing values for group '{group_name}' {title_prefix}:\")\n",
    "        print(group_df[critical_columns].isnull().sum())\n",
    "        for col in critical_columns:\n",
    "            plt.figure(figsize=(6,4))\n",
    "            plt.hist(group_df[col].dropna(), bins=20, edgecolor='black')\n",
    "            plt.title(f\"{title_prefix}: Histogram of {col} (Group: {group_name})\")\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading and preparing data...\")\n",
    "    \n",
    "    \n",
    "    combined_df = load_analysis_data()\n",
    "    \n",
    "    if combined_df is not None:\n",
    "        # Optional: Diagnostics before cleaning\n",
    "        print(\"\\n--- DIAGNOSTICS BEFORE CLEANING ---\")\n",
    "        print(\"\\nOverall summary statistics (critical columns):\")\n",
    "        print(combined_df[['total_baths', 'days_since_last_dressing_change', 'admission_age', 'plt_mean', 'wbc_mean']].describe())\n",
    "        print(\"\\nOverall missing values (critical columns):\")\n",
    "        print(combined_df[['total_baths', 'days_since_last_dressing_change', 'admission_age', 'plt_mean', 'wbc_mean']].isnull().sum())\n",
    "        diagnostic_plots(combined_df, group_col='is_clabsi', title_prefix=\"Before Cleaning\")\n",
    "        \n",
    "        \n",
    "        print(\"\\nCleaning data...\")\n",
    "        cleaned_df = clean_combined_data(combined_df)\n",
    "        \n",
    "       \n",
    "        print(\"\\n--- DIAGNOSTICS AFTER CLEANING ---\")\n",
    "        print(\"\\nOverall summary statistics (critical columns):\")\n",
    "        print(cleaned_df[['total_baths', 'days_since_last_dressing_change', 'admission_age', 'plt_mean', 'wbc_mean']].describe())\n",
    "        print(\"\\nOverall missing values (critical columns):\")\n",
    "        print(cleaned_df[['total_baths', 'days_since_last_dressing_change', 'admission_age', 'plt_mean', 'wbc_mean']].isnull().sum())\n",
    "        diagnostic_plots(cleaned_df, group_col='is_clabsi', title_prefix=\"After Cleaning\")\n",
    "        \n",
    "        \n",
    "        print(\"\\nPreparing features from cleaned data...\")\n",
    "        X, y, feature_categories = prepare_features(cleaned_df)\n",
    "        \n",
    "        \n",
    "        print(\"\\nCalculating feature importance...\")\n",
    "        importance_df = calculate_feature_importance(X, y)\n",
    "        \n",
    "        # Save the importance results to CSV and display top 20 features\n",
    "        importance_df.to_csv('feature_importance_results.csv', index=False)\n",
    "        print(\"\\nTop 20 Most Important Features:\")\n",
    "        print(importance_df.head(20))\n",
    "        \n",
    "       \n",
    "        def plot_feature_importance(importance_df, top_n=20):\n",
    "            top_features = importance_df.sort_values('composite_score', ascending=False).head(top_n)\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.barh(top_features['feature'], top_features['composite_score'], color='skyblue')\n",
    "            plt.xlabel('Composite Importance Score (Normalized)')\n",
    "            plt.title(f'Top {top_n} Most Important Features')\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.show()\n",
    "        \n",
    "        plot_feature_importance(importance_df, top_n=20)\n",
    "    else:\n",
    "        print(\"Error: Could not load or prepare data properly\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ba100a-fecb-43b0-99f1-b06d54344c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_feature_importance(importance_df, top_n=20):\n",
    "    \n",
    "    top_features = importance_df.sort_values('composite_score', ascending=False).head(top_n)\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(top_features['feature'], top_features['composite_score'], color='skyblue')\n",
    "    plt.xlabel('Composite Importance Score (Normalized)')\n",
    "    plt.title(f'Top {top_n} Most Important Features')\n",
    "    plt.gca().invert_yaxis()  # Highest scores at the top\n",
    "    plt.show()\n",
    "\n",
    "# Plot the top 20 important features\n",
    "plot_feature_importance(importance_df, top_n=20)\n",
    "\n",
    "\n",
    "def select_top_features(importance_df, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Select features whose composite importance score exceeds a threshold.\n",
    "    \"\"\"\n",
    "    # For example, select features with a composite score greater than the threshold.\n",
    "    selected_features = importance_df[importance_df['composite_score'] >= threshold]['feature'].tolist()\n",
    "    print(f\"Selected {len(selected_features)} features with composite score >= {threshold}:\")\n",
    "    print(selected_features)\n",
    "    return selected_features\n",
    "\n",
    "\n",
    "selected_features = select_top_features(importance_df, threshold=0.2)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "X_final = X[selected_features]\n",
    "y_final = y\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, y_final, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train a final XGBoost model\n",
    "final_model = xgb.XGBClassifier(random_state=42, n_estimators=100, learning_rate=0.1)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = final_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nFinal model accuracy with selected features: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99145265-4b71-48db-888b-47ffc0113c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "import joblib\n",
    "import traceback\n",
    "\n",
    "\n",
    "best_params = {\n",
    "    'colsample_bytree': 0.8,\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 7,\n",
    "    'n_estimators': 200,\n",
    "    'subsample': 1.0\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "print(\"Splitting data for model evaluation...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate the XGBoost classifier with the best parameters\n",
    "eval_model = xgb.XGBClassifier(\n",
    "    colsample_bytree=best_params['colsample_bytree'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    subsample=best_params['subsample'],\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "print(\"Training model on the training set...\")\n",
    "eval_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = eval_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, eval_model.predict_proba(X_test)[:, 1])\n",
    "print(\"\\nEvaluation Metrics on Test Data:\")\n",
    "print(\"Test Accuracy: {:.4f}\".format(accuracy))\n",
    "print(\"Test ROC AUC: {:.4f}\".format(roc_auc))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"\\nRetraining final model on the full dataset...\")\n",
    "final_model = xgb.XGBClassifier(\n",
    "    colsample_bytree=best_params['colsample_bytree'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    subsample=best_params['subsample'],\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Train on the full dataset\n",
    "    final_model.fit(X, y)\n",
    "    \n",
    "    # Save the final model to disk (you can choose either joblib or xgboost's save_model)\n",
    "    joblib.dump(final_model, 'final_xgb_model.pkl')\n",
    "    print(\"Final model has been retrained on the full dataset and saved as 'final_xgb_model.pkl'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during final model training or saving: {e}\")\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc8ff59-a8b8-4486-a21a-a9a4d358dbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate the final model with the best hyperparameters you found:\n",
    "# {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'subsample': 1.0}\n",
    "final_model = xgb.XGBClassifier(\n",
    "    colsample_bytree=0.8,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=7,\n",
    "    n_estimators=200,\n",
    "    subsample=1.0,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,  # Suppress deprecation warnings in newer versions\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Train the model on the training set\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = final_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, final_model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "print(\"Final Model Test Accuracy: {:.4f}\".format(accuracy))\n",
    "print(\"Final Model Test ROC AUC: {:.4f}\".format(roc_auc))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfce1af4-8ead-479e-bfe2-503cab798f28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
